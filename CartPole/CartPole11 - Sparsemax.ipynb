{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-20 01:13:37,193] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "dis = 0.99\n",
    "REPLAY_MEMORY = 10000\n",
    "batch_size = 128\n",
    "alpha = 0.6\n",
    "beta_init = 0.4\n",
    "eps = 0.01\n",
    "training_step = 20\n",
    "copy_step = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN :\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\") :\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self, h_size=128, l_rate=0.005) :\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            \n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "            layer1 = tf.nn.relu(tf.matmul(self._X, W1))\n",
    "            \n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "            W3 = tf.get_variable(\"W3\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer1, W2))\n",
    "            layer3 = tf.nn.relu(tf.matmul(layer1, W3))\n",
    "            \n",
    "            W_V = tf.get_variable(\"W_V\", shape=[h_size, 1],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "            \n",
    "            W_A = tf.get_variable(\"W_A\", shape=[h_size, self.output_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer(seed = 0))\n",
    "            \n",
    "            self.Value = tf.matmul(layer2, W_V)\n",
    "            self.Advantage = tf.matmul(layer3, W_A)\n",
    "            \n",
    "            self._Qpred = self.Value + self.Advantage - tf.reduce_mean(self.Advantage,\n",
    "                                                                       reduction_indices=1,keep_dims=True)\n",
    "        \n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._WIS = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
    "        #self._WIS = tf.placeholder(shape=[1, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "        #self._loss = tf.reduce_mean(tf.multiply(self._WIS, tf.square(self._Y - self._Qpred)))\n",
    "        #self._loss = tf.reduce_mean(self._WIS * tf.square(self._Y - self._Qpred))\n",
    "        \n",
    "        self._train = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(self._loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1,self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X : x})\n",
    "    \n",
    "    def update(self, x_stack, y_stack, w_stack):\n",
    "        return self.session.run([self._loss, self._train],\n",
    "                                feed_dict={self._X : x_stack, self._Y : y_stack, self._WIS : w_stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sparsetau(x):\n",
    "    x = np.array(x)\n",
    "    sorted_x = np.sort(x)[::-1]\n",
    "    S = np.array([])\n",
    "    for i in range(0,len(x)):\n",
    "        if 1+(i+1)*sorted_x[i] >= (sorted_x[0:(i+1)]).sum():\n",
    "            S = np.append(S,sorted_x[i])\n",
    "    tau = (S.sum() - 1)/S.size\n",
    "    return tau, S\n",
    "\n",
    "def sparsedist(x, scale = 1):\n",
    "    x = np.array(x/scale)\n",
    "    tau, _ = sparsetau(x)\n",
    "    p = x - tau\n",
    "    p[p<0] = 0\n",
    "    if p.sum() > 0.0:\n",
    "        p = p/p.sum()\n",
    "    else:\n",
    "        p = np.ones_like(x)/x.shape[0];\n",
    "    return p\n",
    "\n",
    "def sparsemax(x,scale = 1):\n",
    "    x = np.array(x/scale)\n",
    "    tau, S = sparsetau(x)\n",
    "    spmax_x = 0.5*(S**2 - tau**2).sum() + 0.5\n",
    "    spmax_x = scale*spmax_x\n",
    "    return spmax_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_train (mainDQN, targetDQN, train_batch, w_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, input_size)\n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "    w_stack = np.empty(0).reshape(0, 0)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        if done :\n",
    "            Q[0,action] = reward\n",
    "        else :\n",
    "            action_mat = targetDQN.predict(next_state)\n",
    "            action_max = sparsemax(action_mat[0])\n",
    "            Q[0,action] = reward + dis * action_max\n",
    "    \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        \n",
    "    for w in w_batch:\n",
    "        w_stack = np.vstack([w])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack, w_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    \n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "    \n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN) :\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True :\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s,reward,done,_ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done :\n",
    "            print (\"Total score : {}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    max_episodes = 500\n",
    "    end_episode = 0\n",
    "    step_count_total = 0\n",
    "    global_step = 0\n",
    "    beta = beta_init\n",
    "    \n",
    "    replay_buffer = deque()\n",
    "    TD_error_list = []\n",
    "    steps_list = []\n",
    "    step_avg_list = []\n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name = \"target\",\n",
    "                                                src_scope_name = \"main\")\n",
    "        sess.run(copy_ops)\n",
    "    \n",
    "        for episode in range(1, max_episodes):\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            TD_error = 0\n",
    "            state = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                action_mat = mainDQN.predict(state)\n",
    "                action_dist = sparsedist(action_mat[0])\n",
    "                action = np.random.choice(len(action_dist),size=1,p=action_dist)\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action[0])\n",
    "                step_count += 1\n",
    "                global_step += 1\n",
    "                \n",
    "                if done:\n",
    "                    if step_count < 200:\n",
    "                        reward = -100\n",
    "                    TD_error = reward\n",
    "                else:\n",
    "                    action_mat = targetDQN.predict(next_state)\n",
    "                    action_max = sparsemax(action_mat[0])\n",
    "                    TD_error = reward + dis * action_max\n",
    "                \n",
    "                TD_error -= np.max(mainDQN.predict(state))\n",
    "                TD_error = pow((abs(TD_error) + eps), alpha)\n",
    "                TD_error_list.append(TD_error)\n",
    "                \n",
    "                if beta < 1:\n",
    "                    beta +=(1 - beta_init)/REPLAY_MEMORY\n",
    "                \n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                    replay_buffer.popleft()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                if global_step > batch_size and global_step % training_step == 0:\n",
    "                    TD_copy = []\n",
    "                    TD_norm_list = []\n",
    "                    TD_accum_list = []\n",
    "                    W_is_list = []\n",
    "\n",
    "                    start = 0\n",
    "                    len_TD = len(TD_error_list)\n",
    "                    if(len_TD > REPLAY_MEMORY):\n",
    "                        start = len_TD - REPLAY_MEMORY\n",
    "                        TD_copy = TD_error_list[start : len_TD]\n",
    "                        len_TD = REPLAY_MEMORY\n",
    "                    else:\n",
    "                        TD_copy = TD_error_list[:]\n",
    "\n",
    "                    sum_TD = sum(TD_copy)\n",
    "                    TD_norm_list = [TD_copy[i] / sum_TD for i in range(len_TD)]\n",
    "                    TD_accum_list = np.cumsum(TD_norm_list)\n",
    "\n",
    "                    #W_is_list = [np.power((REPLAY_MEMORY * TD_norm_list[i]), -beta) for i in range(len_TD)]\n",
    "                    #maxW = np.max(W_is_list)\n",
    "                    #W_is_list = [W_is_list[i] / maxW for i in range(len_TD)]\n",
    "\n",
    "                    W_is_list = np.ones([len(TD_accum_list)])\n",
    "\n",
    "                    minibatch = []\n",
    "                    w_batch = []\n",
    "\n",
    "                    TDT = np.zeros([len(TD_accum_list)])\n",
    "                    for i in range(batch_size):\n",
    "                        check = True\n",
    "                        while check:\n",
    "                            rand_batch = random.random()\n",
    "                            TD_index = np.nonzero(TD_accum_list >= rand_batch)[0][0]\n",
    "                            if TDT[TD_index] == 0:\n",
    "                                TDT[TD_index] = 1\n",
    "                                check = False\n",
    "\n",
    "                        w_batch.append(W_is_list[TD_index])\n",
    "                        minibatch.append(replay_buffer[TD_index])\n",
    "\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch, w_batch)\n",
    "\n",
    "                if global_step % copy_step == 0:\n",
    "                    sess.run(copy_ops)\n",
    "                \n",
    "            #print(\"episode: {}   steps: {}\".format(episode, step_count))\n",
    "            steps_list.append(step_count)\n",
    "            \n",
    "            if episode < 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / episode)\n",
    "                \n",
    "            if episode == 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "                \n",
    "            if episode > 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_count_total -= steps_list[episode - 101]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "            \n",
    "            print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "            end_episode += 1\n",
    "            if step_avg_list[episode - 1] > 195:\n",
    "                break\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        for episode in range(end_episode + 1, max_episodes):\n",
    "            s = env.reset()\n",
    "            reward_sum = 0\n",
    "            while True :\n",
    "                #env.render()\n",
    "                a = np.argmax(mainDQN.predict(s))\n",
    "                s,reward,done,_ = env.step(a)\n",
    "                reward_sum += reward\n",
    "        \n",
    "                if done :\n",
    "                    #print(\"episode: {}   steps: {}\".format(episode, reward_sum))\n",
    "                    steps_list.append(reward_sum)\n",
    "                    step_count_total += steps_list[episode - 1]\n",
    "                    step_count_total -= steps_list[episode - 101]\n",
    "                    step_avg_list.append(step_count_total / 100)\n",
    "                    print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "                    break\n",
    "        \n",
    "        x_values = list(range(1, max_episodes))\n",
    "        y_values = step_avg_list[:]\n",
    "        plt.plot(x_values, y_values, c='green')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-20 01:13:38,938] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1           11\n",
      "2           12\n",
      "3           14\n",
      "4           15\n",
      "5           15\n",
      "6           14\n",
      "7           15\n",
      "8           15\n",
      "9           17\n",
      "10           18\n",
      "11           21\n",
      "12           26\n",
      "13           28\n",
      "14           30\n",
      "15           33\n",
      "16           35\n",
      "17           37\n",
      "18           37\n",
      "19           39\n",
      "20           41\n",
      "21           41\n",
      "22           48\n",
      "23           48\n",
      "24           50\n",
      "25           52\n",
      "26           52\n",
      "27           51\n",
      "28           51\n",
      "29           51\n",
      "30           54\n",
      "31           54\n",
      "32           58\n",
      "33           61\n",
      "34           65\n",
      "35           69\n",
      "36           72\n",
      "37           76\n",
      "38           79\n",
      "39           82\n",
      "40           85\n",
      "41           88\n",
      "42           90\n",
      "43           93\n",
      "44           95\n",
      "45           98\n",
      "46           100\n",
      "47           102\n",
      "48           104\n",
      "49           106\n",
      "50           108\n",
      "51           110\n",
      "52           111\n",
      "53           113\n",
      "54           115\n",
      "55           116\n",
      "56           118\n",
      "57           119\n",
      "58           121\n",
      "59           122\n",
      "60           123\n",
      "61           124\n",
      "62           126\n",
      "63           127\n",
      "64           128\n",
      "65           129\n",
      "66           130\n",
      "67           131\n",
      "68           132\n",
      "69           133\n",
      "70           134\n",
      "71           135\n",
      "72           136\n",
      "73           137\n",
      "74           138\n",
      "75           138\n",
      "76           139\n",
      "77           140\n",
      "78           141\n",
      "79           142\n",
      "80           142\n",
      "81           143\n",
      "82           144\n",
      "83           144\n",
      "84           145\n",
      "85           146\n",
      "86           146\n",
      "87           147\n",
      "88           147\n",
      "89           148\n",
      "90           149\n",
      "91           149\n",
      "92           150\n",
      "93           150\n",
      "94           151\n",
      "95           151\n",
      "96           152\n",
      "97           150\n",
      "98           149\n",
      "99           150\n",
      "100           150\n",
      "101           152\n",
      "102           154\n",
      "103           156\n",
      "104           158\n",
      "105           160\n",
      "106           162\n",
      "107           163\n",
      "108           165\n",
      "109           167\n",
      "110           169\n",
      "111           170\n",
      "112           171\n",
      "113           173\n",
      "114           174\n",
      "115           175\n",
      "116           177\n",
      "117           178\n",
      "118           180\n",
      "119           181\n",
      "120           182\n",
      "121           184\n",
      "122           184\n",
      "123           185\n",
      "124           186\n",
      "125           187\n",
      "126           189\n",
      "127           190\n",
      "128           192\n",
      "129           194\n",
      "130           194\n",
      "131           196\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "132           196.16\n",
      "133           196.8\n",
      "134           196.8\n",
      "135           196.8\n",
      "136           196.8\n",
      "137           196.8\n",
      "138           196.8\n",
      "139           196.8\n",
      "140           196.8\n",
      "141           196.8\n",
      "142           196.8\n",
      "143           196.8\n",
      "144           196.8\n",
      "145           196.8\n",
      "146           196.8\n",
      "147           196.8\n",
      "148           196.8\n",
      "149           196.8\n",
      "150           196.8\n",
      "151           196.8\n",
      "152           196.8\n",
      "153           196.8\n",
      "154           196.8\n",
      "155           196.8\n",
      "156           196.8\n",
      "157           196.8\n",
      "158           196.8\n",
      "159           196.8\n",
      "160           196.8\n",
      "161           196.8\n",
      "162           196.8\n",
      "163           196.8\n",
      "164           196.8\n",
      "165           196.8\n",
      "166           196.8\n",
      "167           196.8\n",
      "168           196.8\n",
      "169           196.8\n",
      "170           196.8\n",
      "171           196.8\n",
      "172           196.8\n",
      "173           196.8\n",
      "174           196.8\n",
      "175           196.8\n",
      "176           196.8\n",
      "177           196.8\n",
      "178           196.8\n",
      "179           196.8\n",
      "180           196.8\n",
      "181           196.8\n",
      "182           196.8\n",
      "183           196.8\n",
      "184           196.8\n",
      "185           194.92\n",
      "186           194.92\n",
      "187           194.92\n",
      "188           194.92\n",
      "189           194.92\n",
      "190           194.92\n",
      "191           194.92\n",
      "192           194.92\n",
      "193           194.92\n",
      "194           193.04\n",
      "195           193.04\n",
      "196           193.09\n",
      "197           194.87\n",
      "198           196.24\n",
      "199           196.24\n",
      "200           196.24\n",
      "201           196.24\n",
      "202           196.24\n",
      "203           196.24\n",
      "204           196.24\n",
      "205           196.24\n",
      "206           196.24\n",
      "207           196.24\n",
      "208           196.24\n",
      "209           196.24\n",
      "210           196.24\n",
      "211           196.24\n",
      "212           196.24\n",
      "213           196.24\n",
      "214           196.24\n",
      "215           196.24\n",
      "216           196.24\n",
      "217           196.24\n",
      "218           196.24\n",
      "219           196.24\n",
      "220           196.24\n",
      "221           196.24\n",
      "222           196.24\n",
      "223           196.24\n",
      "224           196.24\n",
      "225           196.24\n",
      "226           196.24\n",
      "227           196.24\n",
      "228           196.24\n",
      "229           196.24\n",
      "230           196.24\n",
      "231           196.24\n",
      "232           196.24\n",
      "233           196.24\n",
      "234           196.24\n",
      "235           196.24\n",
      "236           196.24\n",
      "237           196.24\n",
      "238           196.24\n",
      "239           196.24\n",
      "240           196.24\n",
      "241           196.24\n",
      "242           196.24\n",
      "243           196.24\n",
      "244           196.24\n",
      "245           196.24\n",
      "246           196.24\n",
      "247           196.24\n",
      "248           196.24\n",
      "249           196.24\n",
      "250           196.24\n",
      "251           196.24\n",
      "252           196.24\n",
      "253           196.24\n",
      "254           196.24\n",
      "255           196.24\n",
      "256           196.24\n",
      "257           196.24\n",
      "258           196.24\n",
      "259           196.24\n",
      "260           196.24\n",
      "261           196.24\n",
      "262           196.24\n",
      "263           196.24\n",
      "264           196.24\n",
      "265           196.24\n",
      "266           196.24\n",
      "267           196.24\n",
      "268           196.24\n",
      "269           196.24\n",
      "270           196.24\n",
      "271           196.24\n",
      "272           196.24\n",
      "273           196.24\n",
      "274           196.24\n",
      "275           196.24\n",
      "276           196.24\n",
      "277           196.24\n",
      "278           196.24\n",
      "279           196.24\n",
      "280           196.24\n",
      "281           196.24\n",
      "282           196.24\n",
      "283           196.24\n",
      "284           196.24\n",
      "285           198.12\n",
      "286           198.12\n",
      "287           198.12\n",
      "288           198.12\n",
      "289           198.12\n",
      "290           198.12\n",
      "291           198.12\n",
      "292           198.12\n",
      "293           198.12\n",
      "294           200.0\n",
      "295           200.0\n",
      "296           200.0\n",
      "297           200.0\n",
      "298           200.0\n",
      "299           200.0\n",
      "300           200.0\n",
      "301           200.0\n",
      "302           200.0\n",
      "303           200.0\n",
      "304           200.0\n",
      "305           200.0\n",
      "306           200.0\n",
      "307           200.0\n",
      "308           200.0\n",
      "309           200.0\n",
      "310           200.0\n",
      "311           200.0\n",
      "312           200.0\n",
      "313           200.0\n",
      "314           200.0\n",
      "315           200.0\n",
      "316           200.0\n",
      "317           200.0\n",
      "318           200.0\n",
      "319           200.0\n",
      "320           200.0\n",
      "321           200.0\n",
      "322           200.0\n",
      "323           200.0\n",
      "324           200.0\n",
      "325           200.0\n",
      "326           200.0\n",
      "327           200.0\n",
      "328           200.0\n",
      "329           198.12\n",
      "330           198.12\n",
      "331           198.12\n",
      "332           198.12\n",
      "333           198.12\n",
      "334           198.12\n",
      "335           198.12\n",
      "336           198.12\n",
      "337           198.12\n",
      "338           198.12\n",
      "339           198.12\n",
      "340           198.12\n",
      "341           198.12\n",
      "342           198.12\n",
      "343           198.12\n",
      "344           198.12\n",
      "345           198.12\n",
      "346           198.12\n",
      "347           198.12\n",
      "348           198.12\n",
      "349           198.12\n",
      "350           196.24\n",
      "351           196.24\n",
      "352           196.24\n",
      "353           196.24\n",
      "354           196.24\n",
      "355           196.24\n",
      "356           196.24\n",
      "357           196.24\n",
      "358           196.24\n",
      "359           196.24\n",
      "360           196.24\n",
      "361           196.24\n",
      "362           196.24\n",
      "363           196.24\n",
      "364           196.24\n",
      "365           196.24\n",
      "366           196.24\n",
      "367           196.24\n",
      "368           196.24\n",
      "369           196.24\n",
      "370           196.24\n",
      "371           196.24\n",
      "372           196.24\n",
      "373           196.24\n",
      "374           196.24\n",
      "375           196.24\n",
      "376           196.24\n",
      "377           196.24\n",
      "378           196.24\n",
      "379           196.24\n",
      "380           196.24\n",
      "381           196.24\n",
      "382           196.24\n",
      "383           196.24\n",
      "384           196.24\n",
      "385           196.24\n",
      "386           196.24\n",
      "387           196.24\n",
      "388           196.24\n",
      "389           196.24\n",
      "390           196.24\n",
      "391           196.24\n",
      "392           196.24\n",
      "393           196.24\n",
      "394           196.24\n",
      "395           196.24\n",
      "396           196.24\n",
      "397           196.24\n",
      "398           196.24\n",
      "399           196.24\n",
      "400           196.24\n",
      "401           196.24\n",
      "402           196.24\n",
      "403           196.24\n",
      "404           196.24\n",
      "405           196.24\n",
      "406           196.24\n",
      "407           196.24\n",
      "408           196.24\n",
      "409           196.24\n",
      "410           196.24\n",
      "411           196.24\n",
      "412           196.24\n",
      "413           196.24\n",
      "414           196.24\n",
      "415           196.24\n",
      "416           196.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417           196.24\n",
      "418           196.24\n",
      "419           196.24\n",
      "420           196.24\n",
      "421           196.24\n",
      "422           194.35\n",
      "423           194.35\n",
      "424           194.35\n",
      "425           194.35\n",
      "426           194.35\n",
      "427           194.35\n",
      "428           194.35\n",
      "429           196.23\n",
      "430           196.23\n",
      "431           196.23\n",
      "432           196.23\n",
      "433           196.23\n",
      "434           196.23\n",
      "435           196.23\n",
      "436           196.23\n",
      "437           196.23\n",
      "438           196.23\n",
      "439           196.23\n",
      "440           196.23\n",
      "441           196.23\n",
      "442           196.23\n",
      "443           196.23\n",
      "444           196.23\n",
      "445           196.23\n",
      "446           196.23\n",
      "447           196.23\n",
      "448           196.23\n",
      "449           196.23\n",
      "450           196.23\n",
      "451           196.23\n",
      "452           196.23\n",
      "453           196.23\n",
      "454           196.23\n",
      "455           196.23\n",
      "456           196.23\n",
      "457           196.23\n",
      "458           196.23\n",
      "459           196.23\n",
      "460           196.23\n",
      "461           196.23\n",
      "462           196.23\n",
      "463           194.35\n",
      "464           194.35\n",
      "465           194.35\n",
      "466           194.35\n",
      "467           192.47\n",
      "468           192.47\n",
      "469           192.47\n",
      "470           192.47\n",
      "471           192.47\n",
      "472           192.47\n",
      "473           192.47\n",
      "474           192.47\n",
      "475           192.47\n",
      "476           192.47\n",
      "477           192.47\n",
      "478           192.47\n",
      "479           192.47\n",
      "480           192.47\n",
      "481           192.47\n",
      "482           192.47\n",
      "483           192.47\n",
      "484           192.47\n",
      "485           192.47\n",
      "486           192.47\n",
      "487           192.47\n",
      "488           192.47\n",
      "489           192.47\n",
      "490           192.47\n",
      "491           192.47\n",
      "492           192.47\n",
      "493           192.47\n",
      "494           192.47\n",
      "495           192.47\n",
      "496           192.47\n",
      "497           192.47\n",
      "498           192.47\n",
      "499           192.47\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fb86ef325d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
