{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-07 01:52:03,860] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "dis = 0.99\n",
    "REPLAY_MEMORY = 10000\n",
    "batch_size = 256\n",
    "alpha = 0.6\n",
    "beta_init = 0.4\n",
    "eps = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN :\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\") :\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self, h_size=64, l_rate=0.01) :\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            \n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1 = tf.nn.relu(tf.matmul(self._X, W1))\n",
    "            \n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W3 = tf.get_variable(\"W3\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer1, W2))\n",
    "            layer3 = tf.nn.relu(tf.matmul(layer1, W3))\n",
    "            \n",
    "            W_V = tf.get_variable(\"W_V\", shape=[h_size, 1],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            W_A = tf.get_variable(\"W_A\", shape=[h_size, self.output_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.Value = tf.matmul(layer2, W_V)\n",
    "            self.Advantage = tf.matmul(layer3, W_A)\n",
    "            \n",
    "            self._Qpred = self.Value + self.Advantage - tf.reduce_mean(self.Advantage,\n",
    "                                                                       reduction_indices=1,keep_dims=True)\n",
    "        \n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._WIS = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
    "        #self._WIS = tf.placeholder(shape=[1, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "        #self._loss = tf.reduce_mean(tf.multiply(self._WIS, tf.square(self._Y - self._Qpred)))\n",
    "        #self._loss = tf.reduce_mean(self._WIS * tf.square(self._Y - self._Qpred))\n",
    "        \n",
    "        self._train = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(self._loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1,self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X : x})\n",
    "    \n",
    "    def update(self, x_stack, y_stack, w_stack):\n",
    "        return self.session.run([self._loss, self._train],\n",
    "                                feed_dict={self._X : x_stack, self._Y : y_stack, self._WIS : w_stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_train (mainDQN, targetDQN, train_batch, w_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, input_size)\n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "    w_stack = np.empty(0).reshape(0, 0)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        if done :\n",
    "            Q[0,action] = reward\n",
    "        else :\n",
    "            action0 = np.argmax(mainDQN.predict(next_state))\n",
    "            Q[0,action] = reward + dis * (targetDQN.predict(next_state)[0, action0])\n",
    "    \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        \n",
    "    for w in w_batch:\n",
    "        w_stack = np.vstack([w])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack, w_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    \n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "    \n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN) :\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True :\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s,reward,done,_ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done :\n",
    "            print (\"Total score : {}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    max_episodes = 500\n",
    "    end_episode = 0\n",
    "    step_count_total = 0\n",
    "    beta = beta_init\n",
    "    \n",
    "    replay_buffer = deque()\n",
    "    TD_error_list = []\n",
    "    steps_list = []\n",
    "    step_avg_list = []\n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name = \"target\",\n",
    "                                                src_scope_name = \"main\")\n",
    "        sess.run(copy_ops)\n",
    "    \n",
    "        for episode in range(1, max_episodes):\n",
    "            e = 1. / (((episode - 1) / 5) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            TD_error = 0\n",
    "            state = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                #env.render()\n",
    "                if np.random.rand(1) < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                step_count += 1\n",
    "                \n",
    "                if done:\n",
    "                    if step_count < 200:\n",
    "                        reward = -100\n",
    "                    TD_error = reward\n",
    "                else:\n",
    "                    action0 = np.argmax(mainDQN.predict(next_state))\n",
    "                    TD_error = reward + dis * (targetDQN.predict(next_state)[0, action0])\n",
    "                \n",
    "                TD_error -= np.max(mainDQN.predict(state))\n",
    "                TD_error = pow((abs(TD_error) + eps), alpha)\n",
    "                TD_error_list.append(TD_error)\n",
    "                \n",
    "                if beta < 1:\n",
    "                    beta +=(1 - beta_init)/REPLAY_MEMORY\n",
    "                \n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                    replay_buffer.popleft()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                #if step_count_total % 100\n",
    "                \n",
    "            #print(\"episode: {}   steps: {}\".format(episode, step_count))\n",
    "            steps_list.append(step_count)\n",
    "            \n",
    "            if episode < 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / episode)\n",
    "                \n",
    "            if episode == 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "                \n",
    "            if episode > 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_count_total -= steps_list[episode - 101]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "            \n",
    "            sample = 0\n",
    "            if step_count_total < batch_size:\n",
    "                #sample = step_count_total\n",
    "                sample = step_count_total/2\n",
    "            else:\n",
    "                sample = batch_size\n",
    "            \n",
    "            TD_copy = []\n",
    "            TD_norm_list = []\n",
    "            TD_accum_list = []\n",
    "            W_is_list = []\n",
    "                \n",
    "            start = 0\n",
    "            len_TD = len(TD_error_list)\n",
    "            if(len_TD > REPLAY_MEMORY):\n",
    "                start = len_TD - REPLAY_MEMORY\n",
    "                TD_copy = TD_error_list[start : len_TD]\n",
    "                len_TD = REPLAY_MEMORY\n",
    "            else:\n",
    "                TD_copy = TD_error_list[:]\n",
    "                \n",
    "            sum_TD = sum(TD_copy)\n",
    "            TD_norm_list = [TD_copy[i] / sum_TD for i in range(len_TD)]\n",
    "            TD_accum_list = np.cumsum(TD_norm_list)\n",
    "                \n",
    "            #W_is_list = [np.power((REPLAY_MEMORY * TD_norm_list[i]), -beta) for i in range(len_TD)]\n",
    "            #maxW = np.max(W_is_list)\n",
    "            #W_is_list = [W_is_list[i] / maxW for i in range(len_TD)]\n",
    "                \n",
    "            W_is_list = np.ones([len(TD_accum_list)])\n",
    "                              \n",
    "            minibatch = []\n",
    "            w_batch = []\n",
    "                \n",
    "            TDT = np.zeros([len(TD_accum_list)])\n",
    "            for i in range(sample):\n",
    "                check = True\n",
    "                while check:\n",
    "                    rand_batch = random.random()\n",
    "                    TD_index = np.nonzero(TD_accum_list >= rand_batch)[0][0]\n",
    "                    if TDT[TD_index] == 0:\n",
    "                        TDT[TD_index] = 1\n",
    "                        check = False\n",
    "                    \n",
    "                w_batch.append(W_is_list[TD_index])\n",
    "                minibatch.append(replay_buffer[TD_index])\n",
    "            \n",
    "            loss, _ = replay_train(mainDQN, targetDQN, minibatch, w_batch)\n",
    "            sess.run(copy_ops)\n",
    "                \n",
    "            print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "            end_episode += 1\n",
    "            if step_avg_list[episode - 1] > 195:\n",
    "                break\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        for episode in range(end_episode + 1, max_episodes):\n",
    "            s = env.reset()\n",
    "            reward_sum = 0\n",
    "            while True :\n",
    "                #env.render()\n",
    "                a = np.argmax(mainDQN.predict(s))\n",
    "                s,reward,done,_ = env.step(a)\n",
    "                reward_sum += reward\n",
    "        \n",
    "                if done :\n",
    "                    #print(\"episode: {}   steps: {}\".format(episode, reward_sum))\n",
    "                    steps_list.append(reward_sum)\n",
    "                    step_count_total += steps_list[episode - 1]\n",
    "                    step_count_total -= steps_list[episode - 101]\n",
    "                    step_avg_list.append(step_count_total / 100)\n",
    "                    print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "                    break\n",
    "        \n",
    "        x_values = list(range(1, max_episodes))\n",
    "        y_values = step_avg_list[:]\n",
    "        plt.plot(x_values, y_values, c='green')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-07 01:52:05,425] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1           15\n",
      "2           13\n",
      "3           14\n",
      "4           13\n",
      "5           15\n",
      "6           15\n",
      "7           14\n",
      "8           14\n",
      "9           14\n",
      "10           13\n",
      "11           13\n",
      "12           13\n",
      "13           13\n",
      "14           13\n",
      "15           12\n",
      "16           12\n",
      "17           12\n",
      "18           12\n",
      "19           12\n",
      "20           12\n",
      "21           12\n",
      "22           12\n",
      "23           15\n",
      "24           23\n",
      "25           29\n",
      "26           30\n",
      "27           35\n",
      "28           37\n",
      "29           40\n",
      "30           41\n",
      "31           44\n",
      "32           46\n",
      "33           47\n",
      "34           48\n",
      "35           53\n",
      "36           55\n",
      "37           56\n",
      "38           56\n",
      "39           56\n",
      "40           59\n",
      "41           59\n",
      "42           61\n",
      "43           63\n",
      "44           66\n",
      "45           66\n",
      "46           67\n",
      "47           67\n",
      "48           67\n",
      "49           69\n",
      "50           70\n",
      "51           69\n",
      "52           71\n",
      "53           71\n",
      "54           72\n",
      "55           73\n",
      "56           73\n",
      "57           75\n",
      "58           75\n",
      "59           76\n",
      "60           76\n",
      "61           78\n",
      "62           79\n",
      "63           78\n",
      "64           80\n",
      "65           80\n",
      "66           81\n",
      "67           81\n",
      "68           81\n",
      "69           81\n",
      "70           82\n",
      "71           82\n",
      "72           82\n",
      "73           82\n",
      "74           82\n",
      "75           82\n",
      "76           82\n",
      "77           82\n",
      "78           82\n",
      "79           82\n",
      "80           83\n",
      "81           85\n",
      "82           85\n",
      "83           84\n",
      "84           83\n",
      "85           84\n",
      "86           85\n",
      "87           85\n",
      "88           86\n",
      "89           86\n",
      "90           85\n",
      "91           85\n",
      "92           85\n",
      "93           85\n",
      "94           85\n",
      "95           84\n",
      "96           86\n",
      "97           87\n",
      "98           87\n",
      "99           88\n",
      "100           88\n",
      "101           89\n",
      "102           90\n",
      "103           90\n",
      "104           91\n",
      "105           91\n",
      "106           93\n",
      "107           93\n",
      "108           94\n",
      "109           95\n",
      "110           95\n",
      "111           96\n",
      "112           96\n",
      "113           97\n",
      "114           97\n",
      "115           98\n",
      "116           98\n",
      "117           98\n",
      "118           100\n",
      "119           100\n",
      "120           101\n",
      "121           102\n",
      "122           103\n",
      "123           102\n",
      "124           101\n",
      "125           100\n",
      "126           100\n",
      "127           100\n",
      "128           100\n",
      "129           101\n",
      "130           101\n",
      "131           101\n",
      "132           102\n",
      "133           102\n",
      "134           102\n",
      "135           102\n",
      "136           103\n",
      "137           104\n",
      "138           106\n",
      "139           107\n",
      "140           107\n",
      "141           108\n",
      "142           109\n",
      "143           109\n",
      "144           110\n",
      "145           111\n",
      "146           112\n",
      "147           113\n",
      "148           114\n",
      "149           115\n",
      "150           116\n",
      "151           117\n",
      "152           118\n",
      "153           119\n",
      "154           120\n",
      "155           121\n",
      "156           122\n",
      "157           122\n",
      "158           123\n",
      "159           124\n",
      "160           125\n",
      "161           125\n",
      "162           126\n",
      "163           127\n",
      "164           127\n",
      "165           129\n",
      "166           129\n",
      "167           130\n",
      "168           132\n",
      "169           132\n",
      "170           134\n",
      "171           134\n",
      "172           136\n",
      "173           137\n",
      "174           138\n",
      "175           139\n",
      "176           140\n",
      "177           142\n",
      "178           143\n",
      "179           144\n",
      "180           144\n",
      "181           144\n",
      "182           145\n",
      "183           147\n",
      "184           148\n",
      "185           149\n",
      "186           150\n",
      "187           150\n",
      "188           151\n",
      "189           152\n",
      "190           154\n",
      "191           155\n",
      "192           156\n",
      "193           158\n",
      "194           159\n",
      "195           160\n",
      "196           160\n",
      "197           160\n",
      "198           161\n",
      "199           161\n",
      "200           163\n",
      "201           163\n",
      "202           164\n",
      "203           166\n",
      "204           167\n",
      "205           169\n",
      "206           169\n",
      "207           170\n",
      "208           171\n",
      "209           173\n",
      "210           174\n",
      "211           175\n",
      "212           177\n",
      "213           178\n",
      "214           179\n",
      "215           181\n",
      "216           182\n",
      "217           184\n",
      "218           184\n",
      "219           186\n",
      "220           187\n",
      "221           188\n",
      "222           189\n",
      "223           191\n",
      "224           192\n",
      "225           193\n",
      "226           195\n",
      "227           195\n",
      "228           196\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "229           196.2\n",
      "230           197.06\n",
      "231           197.69\n",
      "232           198.37\n",
      "233           199.47\n",
      "234           200.0\n",
      "235           200.0\n",
      "236           200.0\n",
      "237           200.0\n",
      "238           200.0\n",
      "239           200.0\n",
      "240           200.0\n",
      "241           200.0\n",
      "242           200.0\n",
      "243           200.0\n",
      "244           200.0\n",
      "245           200.0\n",
      "246           200.0\n",
      "247           200.0\n",
      "248           200.0\n",
      "249           200.0\n",
      "250           200.0\n",
      "251           200.0\n",
      "252           200.0\n",
      "253           200.0\n",
      "254           200.0\n",
      "255           200.0\n",
      "256           200.0\n",
      "257           200.0\n",
      "258           200.0\n",
      "259           200.0\n",
      "260           200.0\n",
      "261           200.0\n",
      "262           200.0\n",
      "263           200.0\n",
      "264           200.0\n",
      "265           200.0\n",
      "266           200.0\n",
      "267           200.0\n",
      "268           200.0\n",
      "269           200.0\n",
      "270           200.0\n",
      "271           200.0\n",
      "272           200.0\n",
      "273           200.0\n",
      "274           200.0\n",
      "275           200.0\n",
      "276           200.0\n",
      "277           200.0\n",
      "278           200.0\n",
      "279           200.0\n",
      "280           200.0\n",
      "281           200.0\n",
      "282           200.0\n",
      "283           200.0\n",
      "284           200.0\n",
      "285           200.0\n",
      "286           200.0\n",
      "287           200.0\n",
      "288           200.0\n",
      "289           200.0\n",
      "290           200.0\n",
      "291           200.0\n",
      "292           200.0\n",
      "293           200.0\n",
      "294           200.0\n",
      "295           200.0\n",
      "296           200.0\n",
      "297           200.0\n",
      "298           200.0\n",
      "299           200.0\n",
      "300           200.0\n",
      "301           200.0\n",
      "302           200.0\n",
      "303           200.0\n",
      "304           200.0\n",
      "305           200.0\n",
      "306           200.0\n",
      "307           200.0\n",
      "308           200.0\n",
      "309           200.0\n",
      "310           200.0\n",
      "311           200.0\n",
      "312           200.0\n",
      "313           200.0\n",
      "314           200.0\n",
      "315           200.0\n",
      "316           200.0\n",
      "317           200.0\n",
      "318           200.0\n",
      "319           200.0\n",
      "320           200.0\n",
      "321           200.0\n",
      "322           200.0\n",
      "323           200.0\n",
      "324           200.0\n",
      "325           200.0\n",
      "326           200.0\n",
      "327           200.0\n",
      "328           200.0\n",
      "329           200.0\n",
      "330           200.0\n",
      "331           200.0\n",
      "332           200.0\n",
      "333           200.0\n",
      "334           200.0\n",
      "335           200.0\n",
      "336           200.0\n",
      "337           200.0\n",
      "338           200.0\n",
      "339           200.0\n",
      "340           200.0\n",
      "341           200.0\n",
      "342           200.0\n",
      "343           200.0\n",
      "344           200.0\n",
      "345           200.0\n",
      "346           200.0\n",
      "347           200.0\n",
      "348           200.0\n",
      "349           200.0\n",
      "350           200.0\n",
      "351           200.0\n",
      "352           200.0\n",
      "353           200.0\n",
      "354           200.0\n",
      "355           200.0\n",
      "356           200.0\n",
      "357           200.0\n",
      "358           200.0\n",
      "359           200.0\n",
      "360           200.0\n",
      "361           200.0\n",
      "362           200.0\n",
      "363           200.0\n",
      "364           200.0\n",
      "365           200.0\n",
      "366           200.0\n",
      "367           200.0\n",
      "368           200.0\n",
      "369           200.0\n",
      "370           200.0\n",
      "371           200.0\n",
      "372           200.0\n",
      "373           200.0\n",
      "374           200.0\n",
      "375           200.0\n",
      "376           200.0\n",
      "377           200.0\n",
      "378           200.0\n",
      "379           200.0\n",
      "380           200.0\n",
      "381           200.0\n",
      "382           200.0\n",
      "383           200.0\n",
      "384           200.0\n",
      "385           200.0\n",
      "386           200.0\n",
      "387           200.0\n",
      "388           200.0\n",
      "389           200.0\n",
      "390           200.0\n",
      "391           200.0\n",
      "392           200.0\n",
      "393           200.0\n",
      "394           200.0\n",
      "395           200.0\n",
      "396           200.0\n",
      "397           200.0\n",
      "398           200.0\n",
      "399           200.0\n",
      "400           200.0\n",
      "401           200.0\n",
      "402           200.0\n",
      "403           200.0\n",
      "404           200.0\n",
      "405           200.0\n",
      "406           200.0\n",
      "407           200.0\n",
      "408           200.0\n",
      "409           200.0\n",
      "410           200.0\n",
      "411           200.0\n",
      "412           200.0\n",
      "413           200.0\n",
      "414           200.0\n",
      "415           200.0\n",
      "416           200.0\n",
      "417           200.0\n",
      "418           200.0\n",
      "419           200.0\n",
      "420           200.0\n",
      "421           200.0\n",
      "422           200.0\n",
      "423           200.0\n",
      "424           200.0\n",
      "425           200.0\n",
      "426           200.0\n",
      "427           200.0\n",
      "428           200.0\n",
      "429           200.0\n",
      "430           200.0\n",
      "431           200.0\n",
      "432           200.0\n",
      "433           200.0\n",
      "434           200.0\n",
      "435           200.0\n",
      "436           200.0\n",
      "437           200.0\n",
      "438           200.0\n",
      "439           200.0\n",
      "440           200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441           200.0\n",
      "442           200.0\n",
      "443           200.0\n",
      "444           200.0\n",
      "445           200.0\n",
      "446           200.0\n",
      "447           200.0\n",
      "448           200.0\n",
      "449           200.0\n",
      "450           200.0\n",
      "451           200.0\n",
      "452           200.0\n",
      "453           200.0\n",
      "454           200.0\n",
      "455           200.0\n",
      "456           200.0\n",
      "457           200.0\n",
      "458           200.0\n",
      "459           200.0\n",
      "460           200.0\n",
      "461           200.0\n",
      "462           200.0\n",
      "463           200.0\n",
      "464           200.0\n",
      "465           200.0\n",
      "466           200.0\n",
      "467           200.0\n",
      "468           200.0\n",
      "469           200.0\n",
      "470           200.0\n",
      "471           200.0\n",
      "472           200.0\n",
      "473           200.0\n",
      "474           200.0\n",
      "475           200.0\n",
      "476           200.0\n",
      "477           200.0\n",
      "478           200.0\n",
      "479           200.0\n",
      "480           200.0\n",
      "481           200.0\n",
      "482           200.0\n",
      "483           200.0\n",
      "484           200.0\n",
      "485           200.0\n",
      "486           200.0\n",
      "487           200.0\n",
      "488           200.0\n",
      "489           200.0\n",
      "490           200.0\n",
      "491           200.0\n",
      "492           200.0\n",
      "493           200.0\n",
      "494           200.0\n",
      "495           200.0\n",
      "496           200.0\n",
      "497           200.0\n",
      "498           200.0\n",
      "499           200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f59e2d10490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
