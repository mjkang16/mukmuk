{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-09 01:27:34,621] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed = 0\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = env.action_space.n\n",
    "\n",
    "dis = 0.99\n",
    "REPLAY_MEMORY = 10000\n",
    "batch_size = 256\n",
    "alpha = 0.6\n",
    "beta_init = 0.4\n",
    "eps = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN :\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\") :\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self, h_size=64, l_rate=0.01) :\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            \n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1 = tf.nn.relu(tf.matmul(self._X, W1))\n",
    "            \n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W3 = tf.get_variable(\"W3\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer1, W2))\n",
    "            layer3 = tf.nn.relu(tf.matmul(layer1, W3))\n",
    "            \n",
    "            W_V = tf.get_variable(\"W_V\", shape=[h_size, 1],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            W_A = tf.get_variable(\"W_A\", shape=[h_size, self.output_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.Value = tf.matmul(layer2, W_V)\n",
    "            self.Advantage = tf.matmul(layer3, W_A)\n",
    "            \n",
    "            self._Qpred = self.Value + self.Advantage - tf.reduce_mean(self.Advantage,\n",
    "                                                                       reduction_indices=1,keep_dims=True)\n",
    "        \n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._WIS = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
    "        #self._WIS = tf.placeholder(shape=[1, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "        #self._loss = tf.reduce_mean(tf.multiply(self._WIS, tf.square(self._Y - self._Qpred)))\n",
    "        #self._loss = tf.reduce_mean(self._WIS * tf.square(self._Y - self._Qpred))\n",
    "        \n",
    "        self._train = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(self._loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1,self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X : x})\n",
    "    \n",
    "    def update(self, x_stack, y_stack, w_stack):\n",
    "        return self.session.run([self._loss, self._train],\n",
    "                                feed_dict={self._X : x_stack, self._Y : y_stack, self._WIS : w_stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, scale = 1):\n",
    "    x = np.array(x)/scale\n",
    "    max_x = np.max(x)\n",
    "    e_x = np.exp(x - max_x)\n",
    "    p = e_x/e_x.sum()\n",
    "    p = p/p.sum()\n",
    "    return p\n",
    "\n",
    "def softV(x, scale = 1):\n",
    "    x = np.array(x)/scale\n",
    "    max_x = np.max(x)\n",
    "    e_x = np.exp(x - max_x)\n",
    "    e_sum = e_x.sum()\n",
    "    e_sum = scale * (np.log(e_sum) + max_x)\n",
    "    return e_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_train (mainDQN, targetDQN, train_batch, w_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, input_size)\n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "    w_stack = np.empty(0).reshape(0, 0)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        if done :\n",
    "            Q[0,action] = reward\n",
    "        else :\n",
    "            action_mat = targetDQN.predict(next_state)\n",
    "            action_V = softV(action_mat[0])\n",
    "            Q[0,action] = reward + dis * action_V\n",
    "            \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        \n",
    "    for w in w_batch:\n",
    "        w_stack = np.vstack([w])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack, w_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    \n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "    \n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN) :\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True :\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s,reward,done,_ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done :\n",
    "            print (\"Total score : {}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    max_episodes = 500\n",
    "    end_episode = 0\n",
    "    step_count_total = 0\n",
    "    beta = beta_init\n",
    "    \n",
    "    replay_buffer = deque()\n",
    "    TD_error_list = []\n",
    "    steps_list = []\n",
    "    step_avg_list = []\n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        \n",
    "        copy_ops = get_copy_var_ops(dest_scope_name = \"target\",\n",
    "                                                src_scope_name = \"main\")\n",
    "        sess.run(copy_ops)\n",
    "    \n",
    "        for episode in range(1, max_episodes):\n",
    "            #e = 1. / (((episode - 1) / 5) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            TD_error = 0\n",
    "            state = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                action_mat = mainDQN.predict(state)\n",
    "                action_max = softmax(action_mat[0])\n",
    "                action_max = np.cumsum(action_max)\n",
    "                np.random.choice(1,size=2,p=[0.6,0.4])\n",
    "                rand_batch = random.random()\n",
    "                AC_index = np.nonzero(action_max >= rand_batch)[0][0]\n",
    "                action = AC_index        \n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                step_count += 1\n",
    "                \n",
    "                if done:\n",
    "                    if step_count < 200:\n",
    "                        reward = -100\n",
    "                    TD_error = reward\n",
    "                else:\n",
    "                    action_mat = targetDQN.predict(next_state)\n",
    "                    action_V = softV(action_mat[0])\n",
    "                    TD_error = reward + dis * action_V\n",
    "                    \n",
    "                TD_error -= np.max(mainDQN.predict(state))\n",
    "                TD_error = pow((abs(TD_error) + eps), alpha)\n",
    "                TD_error_list.append(TD_error)\n",
    "                \n",
    "                if beta < 1:\n",
    "                    beta +=(1 - beta_init)/REPLAY_MEMORY\n",
    "                \n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                    replay_buffer.popleft()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "            #print(\"episode: {}   steps: {}\".format(episode, step_count))\n",
    "            steps_list.append(step_count)\n",
    "            \n",
    "            if episode < 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / episode)\n",
    "                \n",
    "            if episode == 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "                \n",
    "            if episode > 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_count_total -= steps_list[episode - 101]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "            \n",
    "            sample = 0\n",
    "            if step_count_total < batch_size:\n",
    "                sample = step_count_total\n",
    "            else:\n",
    "                sample = batch_size\n",
    "            \n",
    "            TD_copy = []\n",
    "            TD_norm_list = []\n",
    "            TD_accum_list = []\n",
    "            W_is_list = []\n",
    "                \n",
    "            start = 0\n",
    "            len_TD = len(TD_error_list)\n",
    "            if(len_TD > REPLAY_MEMORY):\n",
    "                start = len_TD - REPLAY_MEMORY\n",
    "                TD_copy = TD_error_list[start : len_TD]\n",
    "                len_TD = REPLAY_MEMORY\n",
    "            else:\n",
    "                TD_copy = TD_error_list[:]\n",
    "                \n",
    "            sum_TD = sum(TD_copy)\n",
    "            TD_norm_list = [TD_copy[i] / sum_TD for i in range(len_TD)]\n",
    "            TD_accum_list = np.cumsum(TD_norm_list)\n",
    "                \n",
    "            #W_is_list = [np.power((REPLAY_MEMORY * TD_norm_list[i]), -beta) for i in range(len_TD)]\n",
    "            #maxW = np.max(W_is_list)\n",
    "            #W_is_list = [W_is_list[i] / maxW for i in range(len_TD)]\n",
    "                \n",
    "            W_is_list = np.ones([len(TD_accum_list)])\n",
    "                              \n",
    "            minibatch = []\n",
    "            w_batch = []\n",
    "                \n",
    "            TDT = np.zeros([len(TD_accum_list)])\n",
    "            for i in range(sample):\n",
    "                check = True\n",
    "                while check:\n",
    "                    rand_batch = random.random()\n",
    "                    TD_index = np.nonzero(TD_accum_list >= rand_batch)[0][0]\n",
    "                    if TDT[TD_index] == 0:\n",
    "                        TDT[TD_index] = 1\n",
    "                        check = False\n",
    "                    \n",
    "                w_batch.append(W_is_list[TD_index])\n",
    "                minibatch.append(replay_buffer[TD_index])\n",
    "                    \n",
    "            loss, _ = replay_train(mainDQN, targetDQN, minibatch, w_batch)\n",
    "                \n",
    "            #print (\"Loss :  \", loss)\n",
    "            sess.run(copy_ops)\n",
    "                \n",
    "            print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "            end_episode += 1\n",
    "            if step_avg_list[episode - 1] > 195:\n",
    "                break\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        for episode in range(end_episode + 1, max_episodes):\n",
    "            s = env.reset()\n",
    "            reward_sum = 0\n",
    "            while True :\n",
    "                #env.render()\n",
    "                a = np.argmax(mainDQN.predict(s))\n",
    "                s,reward,done,_ = env.step(a)\n",
    "                reward_sum += reward\n",
    "        \n",
    "                if done :\n",
    "                    #print(\"episode: {}   steps: {}\".format(episode, reward_sum))\n",
    "                    steps_list.append(reward_sum)\n",
    "                    step_count_total += steps_list[episode - 1]\n",
    "                    step_count_total -= steps_list[episode - 101]\n",
    "                    step_avg_list.append(step_count_total / 100)\n",
    "                    print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "                    break\n",
    "        \n",
    "        x_values = list(range(1, max_episodes))\n",
    "        y_values = step_avg_list[:]\n",
    "        plt.plot(x_values, y_values, c='green')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-09 01:27:36,427] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1           12\n",
      "2           13\n",
      "3           15\n",
      "4           28\n",
      "5           28\n",
      "6           26\n",
      "7           25\n",
      "8           23\n",
      "9           22\n",
      "10           22\n",
      "11           22\n",
      "12           22\n",
      "13           23\n",
      "14           24\n",
      "15           24\n",
      "16           24\n",
      "17           25\n",
      "18           24\n",
      "19           25\n",
      "20           28\n",
      "21           29\n",
      "22           30\n",
      "23           33\n",
      "24           32\n",
      "25           32\n",
      "26           35\n",
      "27           37\n",
      "28           38\n",
      "29           38\n",
      "30           39\n",
      "31           39\n",
      "32           39\n",
      "33           39\n",
      "34           40\n",
      "35           42\n",
      "36           42\n",
      "37           41\n",
      "38           41\n",
      "39           42\n",
      "40           42\n",
      "41           42\n",
      "42           43\n",
      "43           43\n",
      "44           44\n",
      "45           45\n",
      "46           45\n",
      "47           47\n",
      "48           48\n",
      "49           49\n",
      "50           50\n",
      "51           49\n",
      "52           51\n",
      "53           51\n",
      "54           52\n",
      "55           52\n",
      "56           54\n",
      "57           55\n",
      "58           55\n",
      "59           56\n",
      "60           58\n",
      "61           60\n",
      "62           62\n",
      "63           64\n",
      "64           66\n",
      "65           68\n",
      "66           70\n",
      "67           72\n",
      "68           74\n",
      "69           76\n",
      "70           78\n",
      "71           80\n",
      "72           81\n",
      "73           83\n",
      "74           84\n",
      "75           86\n",
      "76           87\n",
      "77           89\n",
      "78           90\n",
      "79           92\n",
      "80           93\n",
      "81           94\n",
      "82           96\n",
      "83           97\n",
      "84           98\n",
      "85           99\n",
      "86           100\n",
      "87           102\n",
      "88           103\n",
      "89           104\n",
      "90           105\n",
      "91           106\n",
      "92           107\n",
      "93           108\n",
      "94           109\n",
      "95           110\n",
      "96           111\n",
      "97           112\n",
      "98           113\n",
      "99           113\n",
      "100           114\n",
      "101           116\n",
      "102           118\n",
      "103           120\n",
      "104           121\n",
      "105           123\n",
      "106           125\n",
      "107           127\n",
      "108           128\n",
      "109           130\n",
      "110           132\n",
      "111           134\n",
      "112           136\n",
      "113           137\n",
      "114           139\n",
      "115           141\n",
      "116           142\n",
      "117           144\n",
      "118           146\n",
      "119           147\n",
      "120           149\n",
      "121           150\n",
      "122           152\n",
      "123           153\n",
      "124           154\n",
      "125           156\n",
      "126           157\n",
      "127           158\n",
      "128           160\n",
      "129           161\n",
      "130           163\n",
      "131           164\n",
      "132           166\n",
      "133           167\n",
      "134           169\n",
      "135           170\n",
      "136           171\n",
      "137           173\n",
      "138           174\n",
      "139           176\n",
      "140           177\n",
      "141           179\n",
      "142           180\n",
      "143           182\n",
      "144           183\n",
      "145           184\n",
      "146           185\n",
      "147           186\n",
      "148           187\n",
      "149           188\n",
      "150           189\n",
      "151           191\n",
      "152           192\n",
      "153           193\n",
      "154           194\n",
      "155           195\n",
      "156           196\n",
      "--------------------------------------------------\n",
      "--------------------------------------------------\n",
      "157           197.37\n",
      "158           198.42\n",
      "159           199.46\n",
      "160           199.95\n",
      "161           200.0\n",
      "162           200.0\n",
      "163           200.0\n",
      "164           200.0\n",
      "165           200.0\n",
      "166           200.0\n",
      "167           200.0\n",
      "168           200.0\n",
      "169           200.0\n",
      "170           200.0\n",
      "171           200.0\n",
      "172           200.0\n",
      "173           200.0\n",
      "174           200.0\n",
      "175           200.0\n",
      "176           200.0\n",
      "177           200.0\n",
      "178           200.0\n",
      "179           200.0\n",
      "180           200.0\n",
      "181           200.0\n",
      "182           200.0\n",
      "183           200.0\n",
      "184           200.0\n",
      "185           200.0\n",
      "186           200.0\n",
      "187           200.0\n",
      "188           200.0\n",
      "189           200.0\n",
      "190           200.0\n",
      "191           200.0\n",
      "192           200.0\n",
      "193           200.0\n",
      "194           200.0\n",
      "195           200.0\n",
      "196           200.0\n",
      "197           200.0\n",
      "198           200.0\n",
      "199           200.0\n",
      "200           200.0\n",
      "201           200.0\n",
      "202           200.0\n",
      "203           200.0\n",
      "204           200.0\n",
      "205           200.0\n",
      "206           200.0\n",
      "207           200.0\n",
      "208           200.0\n",
      "209           200.0\n",
      "210           200.0\n",
      "211           200.0\n",
      "212           200.0\n",
      "213           200.0\n",
      "214           200.0\n",
      "215           200.0\n",
      "216           200.0\n",
      "217           200.0\n",
      "218           200.0\n",
      "219           200.0\n",
      "220           200.0\n",
      "221           200.0\n",
      "222           200.0\n",
      "223           200.0\n",
      "224           200.0\n",
      "225           200.0\n",
      "226           200.0\n",
      "227           200.0\n",
      "228           200.0\n",
      "229           200.0\n",
      "230           200.0\n",
      "231           200.0\n",
      "232           200.0\n",
      "233           200.0\n",
      "234           200.0\n",
      "235           200.0\n",
      "236           200.0\n",
      "237           200.0\n",
      "238           200.0\n",
      "239           200.0\n",
      "240           200.0\n",
      "241           200.0\n",
      "242           200.0\n",
      "243           200.0\n",
      "244           200.0\n",
      "245           200.0\n",
      "246           200.0\n",
      "247           200.0\n",
      "248           200.0\n",
      "249           200.0\n",
      "250           200.0\n",
      "251           200.0\n",
      "252           200.0\n",
      "253           200.0\n",
      "254           200.0\n",
      "255           200.0\n",
      "256           200.0\n",
      "257           200.0\n",
      "258           200.0\n",
      "259           200.0\n",
      "260           200.0\n",
      "261           200.0\n",
      "262           200.0\n",
      "263           200.0\n",
      "264           200.0\n",
      "265           200.0\n",
      "266           200.0\n",
      "267           200.0\n",
      "268           200.0\n",
      "269           200.0\n",
      "270           200.0\n",
      "271           200.0\n",
      "272           200.0\n",
      "273           200.0\n",
      "274           200.0\n",
      "275           200.0\n",
      "276           200.0\n",
      "277           200.0\n",
      "278           200.0\n",
      "279           200.0\n",
      "280           200.0\n",
      "281           200.0\n",
      "282           200.0\n",
      "283           200.0\n",
      "284           200.0\n",
      "285           200.0\n",
      "286           200.0\n",
      "287           200.0\n",
      "288           200.0\n",
      "289           200.0\n",
      "290           200.0\n",
      "291           200.0\n",
      "292           200.0\n",
      "293           200.0\n",
      "294           200.0\n",
      "295           200.0\n",
      "296           200.0\n",
      "297           200.0\n",
      "298           200.0\n",
      "299           200.0\n",
      "300           200.0\n",
      "301           200.0\n",
      "302           200.0\n",
      "303           200.0\n",
      "304           200.0\n",
      "305           200.0\n",
      "306           200.0\n",
      "307           200.0\n",
      "308           200.0\n",
      "309           200.0\n",
      "310           200.0\n",
      "311           200.0\n",
      "312           200.0\n",
      "313           200.0\n",
      "314           200.0\n",
      "315           200.0\n",
      "316           200.0\n",
      "317           200.0\n",
      "318           200.0\n",
      "319           200.0\n",
      "320           200.0\n",
      "321           200.0\n",
      "322           200.0\n",
      "323           200.0\n",
      "324           200.0\n",
      "325           200.0\n",
      "326           200.0\n",
      "327           200.0\n",
      "328           200.0\n",
      "329           200.0\n",
      "330           200.0\n",
      "331           200.0\n",
      "332           200.0\n",
      "333           200.0\n",
      "334           200.0\n",
      "335           200.0\n",
      "336           200.0\n",
      "337           200.0\n",
      "338           200.0\n",
      "339           200.0\n",
      "340           200.0\n",
      "341           200.0\n",
      "342           200.0\n",
      "343           200.0\n",
      "344           200.0\n",
      "345           200.0\n",
      "346           200.0\n",
      "347           200.0\n",
      "348           200.0\n",
      "349           200.0\n",
      "350           200.0\n",
      "351           200.0\n",
      "352           200.0\n",
      "353           200.0\n",
      "354           200.0\n",
      "355           200.0\n",
      "356           200.0\n",
      "357           200.0\n",
      "358           200.0\n",
      "359           200.0\n",
      "360           200.0\n",
      "361           200.0\n",
      "362           200.0\n",
      "363           200.0\n",
      "364           200.0\n",
      "365           200.0\n",
      "366           200.0\n",
      "367           200.0\n",
      "368           200.0\n",
      "369           200.0\n",
      "370           200.0\n",
      "371           200.0\n",
      "372           200.0\n",
      "373           200.0\n",
      "374           200.0\n",
      "375           200.0\n",
      "376           200.0\n",
      "377           200.0\n",
      "378           200.0\n",
      "379           200.0\n",
      "380           200.0\n",
      "381           200.0\n",
      "382           200.0\n",
      "383           200.0\n",
      "384           200.0\n",
      "385           200.0\n",
      "386           200.0\n",
      "387           200.0\n",
      "388           200.0\n",
      "389           200.0\n",
      "390           200.0\n",
      "391           200.0\n",
      "392           200.0\n",
      "393           200.0\n",
      "394           200.0\n",
      "395           200.0\n",
      "396           200.0\n",
      "397           200.0\n",
      "398           200.0\n",
      "399           200.0\n",
      "400           200.0\n",
      "401           200.0\n",
      "402           200.0\n",
      "403           200.0\n",
      "404           200.0\n",
      "405           200.0\n",
      "406           200.0\n",
      "407           200.0\n",
      "408           200.0\n",
      "409           200.0\n",
      "410           200.0\n",
      "411           200.0\n",
      "412           200.0\n",
      "413           200.0\n",
      "414           200.0\n",
      "415           200.0\n",
      "416           200.0\n",
      "417           200.0\n",
      "418           200.0\n",
      "419           200.0\n",
      "420           200.0\n",
      "421           200.0\n",
      "422           200.0\n",
      "423           200.0\n",
      "424           200.0\n",
      "425           200.0\n",
      "426           200.0\n",
      "427           200.0\n",
      "428           200.0\n",
      "429           200.0\n",
      "430           200.0\n",
      "431           200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432           200.0\n",
      "433           200.0\n",
      "434           200.0\n",
      "435           200.0\n",
      "436           200.0\n",
      "437           200.0\n",
      "438           200.0\n",
      "439           200.0\n",
      "440           200.0\n",
      "441           200.0\n",
      "442           200.0\n",
      "443           200.0\n",
      "444           200.0\n",
      "445           200.0\n",
      "446           200.0\n",
      "447           200.0\n",
      "448           200.0\n",
      "449           200.0\n",
      "450           200.0\n",
      "451           200.0\n",
      "452           200.0\n",
      "453           200.0\n",
      "454           200.0\n",
      "455           200.0\n",
      "456           200.0\n",
      "457           200.0\n",
      "458           200.0\n",
      "459           200.0\n",
      "460           200.0\n",
      "461           200.0\n",
      "462           200.0\n",
      "463           200.0\n",
      "464           200.0\n",
      "465           200.0\n",
      "466           200.0\n",
      "467           200.0\n",
      "468           200.0\n",
      "469           200.0\n",
      "470           200.0\n",
      "471           200.0\n",
      "472           200.0\n",
      "473           200.0\n",
      "474           200.0\n",
      "475           200.0\n",
      "476           200.0\n",
      "477           200.0\n",
      "478           200.0\n",
      "479           200.0\n",
      "480           200.0\n",
      "481           200.0\n",
      "482           200.0\n",
      "483           200.0\n",
      "484           200.0\n",
      "485           200.0\n",
      "486           200.0\n",
      "487           200.0\n",
      "488           200.0\n",
      "489           200.0\n",
      "490           200.0\n",
      "491           200.0\n",
      "492           200.0\n",
      "493           200.0\n",
      "494           200.0\n",
      "495           200.0\n",
      "496           200.0\n",
      "497           200.0\n",
      "498           200.0\n",
      "499           200.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc167db9590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
