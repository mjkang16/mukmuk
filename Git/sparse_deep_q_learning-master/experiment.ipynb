{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import wrappers\n",
    "import policy\n",
    "import maxapproxi\n",
    "import network\n",
    "import replaymemory\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class Experiments:\n",
    "    def __init__(self, seed=0, env_name = 'CartPole-v1', action_res=None, dqn_hidden_spec=None, batch_size = 128, \n",
    "                learning_rate=1e-3, sub_opt_max=1,\n",
    "                discount = 0.99, max_epi = 500, max_step = 1000, target_update_period = 5,\n",
    "                replay_memory_size = 10000, eps_decay_rate=0.999, scale=1.,\n",
    "                strategy=\"Epsilon\", backuprule=\"Bellman\"):\n",
    "        # Fix the numpy random seed\n",
    "        rng = np.random.RandomState(seed)\n",
    "        \n",
    "        # Gen environment\n",
    "        env = gym.make(env_name)\n",
    "        eval_env = gym.make(env_name)\n",
    "        \n",
    "        # Get environment information\n",
    "        observ_dim = env.observation_space.high.shape[0]\n",
    "        n_action, conti_action_flag, action_map = self.get_action_information(env, env_name, action_res=action_res)\n",
    "\n",
    "        # Set network spec\n",
    "        if dqn_hidden_spec is None:\n",
    "            dqn_hidden_spec = [\n",
    "                {'dim': 512,'activation': tf.nn.tanh},\n",
    "                {'dim': 512,'activation': tf.nn.tanh}\n",
    "            ]\n",
    "\n",
    "        # Initialize Tensorflow Graph\n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        # Gen value network\n",
    "        value_func = network.Network(input_dim=observ_dim,output_dim=n_action,hidden_spec=dqn_hidden_spec,learning_rate=learning_rate,seed=seed)\n",
    "\n",
    "        # Set session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        session = tf.Session(config=config)\n",
    "        \n",
    "        # Initialize tf variable and old variable of value network\n",
    "        tf.set_random_seed(seed)\n",
    "        session.run(tf.global_variables_initializer())\n",
    "        session.run(value_func.update_ops)\n",
    "        \n",
    "        # Gen replay memory\n",
    "        replay_memory = replaymemory.ReplayMemory(rng=rng,memory_size=replay_memory_size)\n",
    "        \n",
    "        # Gen policy function\n",
    "        policy_func = policy.Policy(n_action,rng=rng,strategy=strategy,eps_decay_rate=eps_decay_rate,scale=scale)\n",
    "\n",
    "        # Store All Variable to Class\n",
    "        self.seed=seed\n",
    "        self.env_name=env_name\n",
    "        self.action_res=action_res\n",
    "        self.dqn_hidden_spec=dqn_hidden_spec\n",
    "        self.batch_size=batch_size\n",
    "        self.learning_rate=learning_rate\n",
    "        self.discount=discount\n",
    "        self.max_epi=max_epi\n",
    "        self.max_step=max_step\n",
    "        self.target_update_period=target_update_period\n",
    "        self.replay_memory_size=replay_memory_size\n",
    "        self.eps_decay_rate=eps_decay_rate\n",
    "        self.strategy=strategy\n",
    "        self.backuprule=backuprule\n",
    "        self.sub_opt_max=sub_opt_max\n",
    "        \n",
    "        self.observ_dim=observ_dim\n",
    "        self.n_action=n_action\n",
    "        self.conti_action_flag=conti_action_flag\n",
    "        self.action_map=action_map\n",
    "        \n",
    "        self.env=env\n",
    "        self.eval_env=eval_env\n",
    "        self.value_func=value_func\n",
    "        self.replay_memory=replay_memory\n",
    "        self.policy_func=policy_func\n",
    "        self.session = session\n",
    "        self.config = config\n",
    "        self.scale = scale\n",
    "        \n",
    "        \n",
    "    def get_action_information(self, env, env_name, action_res=None):\n",
    "        action_map = []\n",
    "        if isinstance(env.action_space, gym.spaces.Box):\n",
    "            conti_action_flag = True\n",
    "            if env_name == \"Pendulum-v0\" or env_name == \"InvertedPendulum-v1\" or env_name == \"MountainCarContinuous-v0\" or env_name == \"InvertedDoublePendulum-v1\":\n",
    "                action_map = np.linspace(env.action_space.low[0],env.action_space.high[0],num=action_res)\n",
    "            elif env_name == \"Reacher-v1\":\n",
    "                action_map = np.zeros([np.prod(action_res), 2])\n",
    "                u = np.linspace(env.action_space.low[0], env.action_space.high[0], num=action_res[0])\n",
    "                v = np.linspace(env.action_space.low[1], env.action_space.high[1], num=action_res[1])\n",
    "                for i in range(action_res[0]):\n",
    "                    for j in range(action_res[1]):\n",
    "                        s = action_res[1] * i + j\n",
    "                        action_map[s, :] = [u[i], v[j]]\n",
    "            elif env_name == \"Swimmer-v1\" or env_name == \"LunarLanderContinuous-v2\" or env_name == \"MultiGoal-v0\":\n",
    "                action_map = np.zeros([np.prod(action_res), 2])\n",
    "                u = np.linspace(env.action_space.low[0], env.action_space.high[0], num=action_res[0])\n",
    "                v = np.linspace(env.action_space.low[1], env.action_space.high[1], num=action_res[1])\n",
    "                for i in range(action_res[0]):\n",
    "                    for j in range(action_res[1]):\n",
    "                        s = action_res[1] * i + j\n",
    "                        action_map[s, :] = [u[i], v[j]]\n",
    "            elif env_name == \"Hopper-v1\":\n",
    "                action_map = np.zeros([np.prod(action_res), 3])\n",
    "                u = np.linspace(env.action_space.low[0], env.action_space.high[0], num=action_res[0])\n",
    "                v = np.linspace(env.action_space.low[1], env.action_space.high[1], num=action_res[1])\n",
    "                w = np.linspace(env.action_space.low[2], env.action_space.high[2], num=action_res[2])\n",
    "                for i in range(action_res[0]):\n",
    "                    for j in range(action_res[1]):\n",
    "                        for k in range(action_res[2]):\n",
    "                            s = action_res[2] * action_res[1] * i + action_res[2] * j + k\n",
    "                            action_map[s, :] = [u[i], v[j], w[k]]\n",
    "            elif env_name == \"Walker2d-v1\":\n",
    "                action_map = np.zeros([np.prod(action_res), 6])\n",
    "                x = np.linspace(env.action_space.low[0], env.action_space.high[0], num=action_res[0])\n",
    "                y = np.linspace(env.action_space.low[1], env.action_space.high[1], num=action_res[1])\n",
    "                z = np.linspace(env.action_space.low[2], env.action_space.high[2], num=action_res[2])\n",
    "                u = np.linspace(env.action_space.low[3], env.action_space.high[3], num=action_res[3])\n",
    "                v = np.linspace(env.action_space.low[4], env.action_space.high[4], num=action_res[4])\n",
    "                w = np.linspace(env.action_space.low[5], env.action_space.high[5], num=action_res[5])\n",
    "                for i0 in range(action_res[0]):\n",
    "                    for i1 in range(action_res[1]):\n",
    "                        for i2 in range(action_res[2]):\n",
    "                            for i3 in range(action_res[3]):\n",
    "                                for i4 in range(action_res[4]):\n",
    "                                    for i5 in range(action_res[5]):\n",
    "                                        s = np.prod(action_res[1:]) * i0\n",
    "                                        s += np.prod(action_res[2:]) * i1\n",
    "                                        s += np.prod(action_res[3:]) * i2\n",
    "                                        s += np.prod(action_res[4:]) * i3\n",
    "                                        s += np.prod(action_res[5:]) * i4\n",
    "                                        s += i5\n",
    "                                        action_map[s, :] = [x[i0], y[i1], z[i2], u[i3], v[i4], w[i5]]\n",
    "            else:\n",
    "                print(env.action_space.high.shape[0])\n",
    "            n_action = np.prod(action_res)\n",
    "        elif isinstance(env.action_space, gym.spaces.Discrete):\n",
    "            conti_action_flag = False\n",
    "            n_action = env.action_space.n\n",
    "        else:\n",
    "            raise NotImplementedError(\"{} action spaces are not supported yet.\".format(type(env.action_space)))\n",
    "        return n_action, conti_action_flag, action_map\n",
    "    \n",
    "    def format_experience(self,experience):\n",
    "        states_b, actions_b, rewards_b, states_n_b, done_b = zip(*experience)\n",
    "        states_b = np.array(states_b)\n",
    "        actions_b = np.array(actions_b)\n",
    "        rewards_b = np.array(rewards_b)\n",
    "        states_n_b = np.array(states_n_b)\n",
    "        done_b = np.array(done_b).astype(int)\n",
    "        return states_b, actions_b, rewards_b, states_n_b, done_b\n",
    "\n",
    "    def run(self, display_period=10):\n",
    "        env = self.env\n",
    "        eval_env = self.eval_env\n",
    "        \n",
    "        max_epi = self.max_epi\n",
    "        max_step = self.max_step\n",
    "        value_func = self.value_func\n",
    "        replay_memory = self.replay_memory\n",
    "        policy_func = self.policy_func\n",
    "        session = self.session\n",
    "        conti_action_flag = self.conti_action_flag\n",
    "        action_map = self.action_map\n",
    "        target_update_period=self.target_update_period\n",
    "        discount=self.discount\n",
    "        n_action=self.n_action\n",
    "        backuprule=self.backuprule\n",
    "        \n",
    "        global_step = 0\n",
    "        return_list = np.zeros((max_epi,))\n",
    "\n",
    "        env.seed(self.seed)\n",
    "        eval_env.seed(self.seed)\n",
    "        \n",
    "        max_return = -np.inf\n",
    "        for epi in range(max_epi):\n",
    "            #Training Phase\n",
    "            policy_func.explore = True\n",
    "            total_v_loss = 0\n",
    "            done = False\n",
    "            obs = env.reset()\n",
    "            \n",
    "            for step in range(max_step):\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                fetches, feeds = value_func.get_predictions([obs])\n",
    "                q_value, = session.run(fetches=fetches,feed_dict=feeds)\n",
    "                q_value = q_value[0]\n",
    "\n",
    "                action = policy_func.get_action(q_value)\n",
    "                if conti_action_flag:\n",
    "                    action_val = action_map[action]\n",
    "                else:\n",
    "                    action_val = action\n",
    "\n",
    "                next_obs, reward, done, info = env.step([action_val])\n",
    "                replay_memory.save_experience(obs, action, reward, next_obs, done)\n",
    "                obs = next_obs\n",
    "                \n",
    "                batch_size = self.batch_size\n",
    "                \n",
    "                replay_memory.anneal_per_importance_sampling(step,max_step)\n",
    "                if replay_memory.memory.n_entries >= batch_size:\n",
    "#                     batch_size = replay_memory.memory.n_entries\n",
    "                    for sub_idx in range(self.sub_opt_max):\n",
    "                        idx, priorities, w, experience = replay_memory.retrieve_experience(batch_size)\n",
    "\n",
    "                        states_b, actions_b, rewards_b, states_n_b, done_b = self.format_experience(experience)\n",
    "\n",
    "                        fetches, feeds = value_func.get_predictions(states_n_b)\n",
    "                        q_n_b, = session.run(fetches=fetches,feed_dict=feeds)\n",
    "\n",
    "                        fetches, feeds = value_func.get_predictions_old(states_n_b)\n",
    "                        q_n_target_b, = session.run(fetches=fetches,feed_dict=feeds)\n",
    "\n",
    "                        best_a = np.argmax(q_n_b, axis=1)\n",
    "                        if backuprule == 'Bellman':\n",
    "                            targets_b = rewards_b + (1. - done_b) * discount * q_n_target_b[np.arange(batch_size), best_a]\n",
    "                        elif backuprule == 'SoftBellman':\n",
    "                            targets_b = rewards_b + (1. - done_b) * discount * maxapproxi.logsumexp(q_n_target_b, scale=self.scale)\n",
    "                        elif backuprule == 'SparseBellman':\n",
    "                            targets_b = rewards_b + (1. - done_b) * discount * maxapproxi.sparsemax(q_n_target_b, scale=self.scale)\n",
    "\n",
    "                        fetches, feeds = value_func.get_predictions(states_b)\n",
    "                        targets, = session.run(fetches=fetches,feed_dict=feeds)\n",
    "                        for j, action in enumerate(actions_b):\n",
    "                            targets[j, action] = targets_b[j]\n",
    "\n",
    "                        fetches, feeds = value_func.get_train(states_b,targets, np.transpose(np.tile(w, (n_action, 1))))\n",
    "                        v_loss, errors, _ = session.run(fetches=fetches,feed_dict=feeds)\n",
    "                        errors = errors[np.arange(len(errors)), actions_b]\n",
    "\n",
    "                        replay_memory.update_experience_weight(idx, errors)\n",
    "                        total_v_loss += v_loss/self.sub_opt_max\n",
    "#                     if v_loss > 1e+5:\n",
    "#                         print(q_n_target_b.shape)\n",
    "#                         print(q_n_target_b)\n",
    "#                         print(rewards_b)\n",
    "                \n",
    "                policy_func.update_policy()\n",
    "                global_step += 1\n",
    "                if (global_step%target_update_period)==0:\n",
    "                    session.run(value_func.update_ops)\n",
    "            \n",
    "            policy_func.explore = False\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            obs = eval_env.reset()\n",
    "            while not done:\n",
    "                fetches, feeds = value_func.get_predictions([obs])\n",
    "                q_value, = session.run(fetches=fetches,feed_dict=feeds)\n",
    "                q_value = q_value[0]\n",
    "\n",
    "                action = policy_func.get_action(q_value)\n",
    "                if conti_action_flag:\n",
    "                    action_val = action_map[action]\n",
    "                else:\n",
    "                    action_val = action\n",
    "\n",
    "                next_obs, reward, done, _ = eval_env.step([action_val])\n",
    "                total_reward += reward\n",
    "                obs = next_obs\n",
    "            \n",
    "            if ((epi+1)%100)==0:\n",
    "                eval_env.seed(self.seed)                \n",
    "            \n",
    "            return_list[epi] = total_reward\n",
    "            if epi < 100-1:\n",
    "                avg_return = np.mean(return_list[:epi+1])\n",
    "            else:\n",
    "                avg_return = np.mean(return_list[epi-100+1:epi+1])\n",
    "            \n",
    "            if epi >= display_period-1 and max_return < avg_return:\n",
    "                max_return = avg_return\n",
    "            if ((epi+1)%display_period)==0:\n",
    "                print('[{}/{}] Avg Return {}, Max Return {}, DQN Loss {}, Epsilon {}'.format(epi+1,max_epi,avg_return,max_return,total_v_loss,policy_func.eps))\n",
    "            env.close()\n",
    "        return return_list, max_return\n",
    "    \n",
    "    def evaluation(self,max_eval_epi=100, video_record=False):\n",
    "        env = self.eval_env\n",
    "        if video_record:\n",
    "            def _video_scheduler(episode_id):\n",
    "                return True\n",
    "            env=wrappers.Monitor(env, \"/home/guest/gitproject/sparse_deep_q_learning/\"+self.env_name+\"/\"+self.strategy+\"_\"+self.backuprule, video_callable=_video_scheduler, force=True)\n",
    "        \n",
    "        max_step = self.max_step\n",
    "        value_func = self.value_func\n",
    "        policy_func = self.policy_func\n",
    "        session = self.session\n",
    "        conti_action_flag = self.conti_action_flag\n",
    "        action_map = self.action_map\n",
    "        n_action=self.n_action\n",
    "        \n",
    "        return_list = np.zeros((max_eval_epi,))\n",
    "        \n",
    "        policy_func.explore = False\n",
    "        env.seed(self.seed)\n",
    "        for epi in range(max_eval_epi):\n",
    "            obs = env.reset()\n",
    "\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                fetches, feeds = value_func.get_predictions([obs])\n",
    "                q_value, = session.run(fetches=fetches,feed_dict=feeds)\n",
    "                q_value = q_value[0]\n",
    "\n",
    "                action = policy_func.get_action(q_value)\n",
    "                if conti_action_flag:\n",
    "                    action_val = action_map[action]\n",
    "                else:\n",
    "                    action_val = action\n",
    "\n",
    "                next_obs, reward, done, _ = env.step([action_val])\n",
    "                total_reward += reward\n",
    "                obs = next_obs\n",
    "            return_list[epi] = total_reward\n",
    "        env.close()\n",
    "        print(\"Evaluation Result: {}\".format(np.mean(return_list)))\n",
    "        return return_list\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unbound method run() must be called with Experiments instance as first argument (got nothing instead)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c51ed4a0f178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mExperiments\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unbound method run() must be called with Experiments instance as first argument (got nothing instead)"
     ]
    }
   ],
   "source": [
    "Experiments.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
