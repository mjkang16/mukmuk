{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "input_size = 84*84*4\n",
    "output_size = env.action_space.n\n",
    "\n",
    "HEIGHT = 84\n",
    "WIDTH = 84\n",
    "HISTORY_SIZE = 4\n",
    "\n",
    "dis = 0.99\n",
    "REPLAY_MEMORY = 400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN :\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\") :\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.height = 84\n",
    "        self.width = 84\n",
    "        self.history_size = 4\n",
    "        self.net_name = name\n",
    "        self._build_network()\n",
    "    \n",
    "    def pre_proc(X):\n",
    "        x = np.uint8(resize(rgb2gray(X), (84, 84), mode='reflect') * 255)\n",
    "        return x\n",
    "    \n",
    "    def _build_network(self, l_rate=0.00025) :\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder('float', [None, self.height, self.width, self.history_size])\n",
    "            self.a = tf.placeholder('int64', [None])\n",
    "            \n",
    "            f1 = tf.get_variable(\"f1\", shape=[8, 8, 4, 16],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            f2 = tf.get_variable(\"f2\", shape=[4, 4, 16, 32],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "            w1 = tf.get_variable(\"w1\", shape=[9*9*32, 256],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            w2 = tf.get_variable(\"w2\", shape=[256, self.output_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            c1 = tf.nn.relu(tf.nn.conv2d(self._X, f1, strides=[1, 4, 4, 1], padding=\"VALID\"))\n",
    "            c2 = tf.nn.relu(tf.nn.conv2d(c1, f2, strides=[1, 2, 2, 1], padding=\"VALID\"))\n",
    "            \n",
    "            l1 = tf.reshape(c2, [-1, w1.get_shape().as_list()[0]])\n",
    "            l2 = tf.nn.relu(tf.matmul(l1, w1))\n",
    "            \n",
    "            self._Qpred = tf.matmul(l2, w2)\n",
    "        \n",
    "        action_one_hot = tf.one_hot(self.a, self.output_size, 1.0, 0.0)\n",
    "        q_val = tf.reduce_sum(tf.multiply(self._Qpred, action_one_hot), axis=1)\n",
    "        \n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "    \n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "    \n",
    "        self._train = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(self._loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X : np.reshape(state, [-1, 84, 84, 4])})\n",
    "    \n",
    "    def update(self, x_stack, y_stack):\n",
    "        return self.session.run([self._loss, self._train],\n",
    "                                feed_dict={self._X : np.reshape(x_stack, [-1, 84, 84, 4]), self._Y : y_stack})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_train (mainDQN, targetDQN, train_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, 84, 84, 4)\n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "    \n",
    "    for history, action, reward, done in train_batch:\n",
    "        Q = mainDQN.predict(history[:, :, :4])\n",
    "        \n",
    "        if done :\n",
    "            Q[0,action] = reward\n",
    "        else :\n",
    "            action0 = np.argmax(mainDQN.predict(history[:, :, 1:]))\n",
    "            Q[0,action] = reward + dis * (targetDQN.predict(history[:, :, 1:])[0, action0])\n",
    "        \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack([x_stack, np.reshape(history[:, :, :4], [-1, 84, 84, 4])])\n",
    "    \n",
    "    return mainDQN.update(x_stack, y_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    \n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "    \n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN) :\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True :\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s,reward,done,_ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done :\n",
    "            print (\"Total score : {}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb2gray(X) :\n",
    "    return np.mean(X, axis=2).astype(np.uint8)\n",
    "\n",
    "def conversion_image(X) :\n",
    "    x = np.uint8(resize(rgb2gray(X), (HEIGHT, WIDTH), mode='reflect') * 255)\n",
    "    return x\n",
    "\n",
    "def init_history_conv(history, state) :\n",
    "    for i in range(5):\n",
    "        history[:, :, i] = conversion_image(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    max_episodes = 1000\n",
    "    \n",
    "    replay_buffer = deque()\n",
    "    \n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "        \n",
    "        copy_ops = get_copy_var_ops(dest_scope_name = \"target\",\n",
    "                                   src_scope_name = \"main\")\n",
    "        sess.run(copy_ops)\n",
    "    \n",
    "        \n",
    "        step_count_total = 0\n",
    "        for episode in range(max_episodes):\n",
    "            e = 1. / ((episode / 10) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            state = env.reset()\n",
    "            \n",
    "            history = np.zeros([84, 84, 5], dtype=np.uint8)\n",
    "            init_history_conv(history, state)\n",
    "            \n",
    "            while not done:\n",
    "                env.render()\n",
    "                \n",
    "                if np.random.rand(1) < e:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(np.reshape(history[:, :, :4], [-1, 84, 84, 4])))\n",
    "    \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                history[:, :, 4] = conversion_image(next_state)\n",
    "                \n",
    "                if done:\n",
    "                    reward = -1\n",
    "                \n",
    "                replay_buffer.append((np.copy(history[:,:,:]), action, reward, done))\n",
    "                history[:, :, :4] = history[:, :, 1:]\n",
    "                if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                    replay_buffer.popleft()\n",
    "    \n",
    "                if reward > 0 :\n",
    "                    step_count += reward\n",
    "                if step_count > 100 :\n",
    "                    break\n",
    "                \n",
    "            print(\"episode: {}   steps: {}\".format(episode, step_count))\n",
    "            step_count_total += step_count\n",
    "    \n",
    "            if episode % 5 == 0 and episode > 0:\n",
    "                if step_count_total == 500:\n",
    "                    break\n",
    "                \n",
    "                for _ in range(10):\n",
    "                    minibatch = random.sample(replay_buffer, 8)\n",
    "                    loss, _ = replay_train(mainDQN, targetDQN, minibatch)\n",
    "    \n",
    "                print (\"Loss :  \", loss)\n",
    "                print (\"Step Total 5 :  \", step_count_total)\n",
    "                step_count_total = 0\n",
    "                sess.run(copy_ops)\n",
    "        \n",
    "        print (\"Simulation--------\")\n",
    "        bot_play(mainDQN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
