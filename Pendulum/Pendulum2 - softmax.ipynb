{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-09 02:12:50,087] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "num_actions = 16\n",
    "min_act = -2\n",
    "max_act = 2\n",
    "dis = 0.99\n",
    "REPLAY_MEMORY = 10000\n",
    "batch_size = 256\n",
    "alpha = 0.6\n",
    "beta_init = 0.4\n",
    "eps = 0.01\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN :\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\") :\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self, h_size=64, l_rate=0.01) :\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            \n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1 = tf.nn.relu(tf.matmul(self._X, W1))\n",
    "            \n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W3 = tf.get_variable(\"W3\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer1, W2))\n",
    "            layer3 = tf.nn.relu(tf.matmul(layer1, W3))\n",
    "            \n",
    "            W_V = tf.get_variable(\"W_V\", shape=[h_size, 1],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            W_A = tf.get_variable(\"W_A\", shape=[h_size, self.output_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.Value = tf.matmul(layer2, W_V)\n",
    "            self.Advantage = tf.matmul(layer3, W_A)\n",
    "            \n",
    "            self._Qpred = self.Value + self.Advantage - tf.reduce_mean(self.Advantage,\n",
    "                                                                       reduction_indices=1,keep_dims=True)\n",
    "        \n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._WIS = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
    "        #self._WIS = tf.placeholder(shape=[1, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "        #self._loss = tf.reduce_mean(tf.multiply(self._WIS, tf.square(self._Y - self._Qpred)))\n",
    "        #self._loss = tf.reduce_mean(self._WIS * tf.square(self._Y - self._Qpred))\n",
    "        \n",
    "        self._train = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(self._loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1,self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X : x})\n",
    "    \n",
    "    def update(self, x_stack, y_stack, w_stack):\n",
    "        return self.session.run([self._loss, self._train],\n",
    "                                feed_dict={self._X : x_stack, self._Y : y_stack, self._WIS : w_stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x, scale = 1):\n",
    "    x = np.array(x)/scale\n",
    "    max_x = np.max(x)\n",
    "    e_x = np.exp(x - max_x)\n",
    "    p = e_x/e_x.sum()\n",
    "    p = p/p.sum()\n",
    "    return p\n",
    "\n",
    "def softV(x, scale = 1):\n",
    "    x = np.array(x)/scale\n",
    "    max_x = np.max(x)\n",
    "    e_x = np.exp(x - max_x)\n",
    "    e_sum = e_x.sum()\n",
    "    e_sum = scale * (np.log(e_sum) + max_x)\n",
    "    return e_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_action(a):\n",
    "    return min_act + a * (max_act - min_act) / (num_actions - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_train (mainDQN, targetDQN, train_batch, w_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, input_size)\n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "    w_stack = np.empty(0).reshape(0, 0)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        if done :\n",
    "            Q[0,action] = reward\n",
    "        else :\n",
    "            action_mat = targetDQN.predict(next_state)\n",
    "            action_V = softV(action_mat[0])\n",
    "            Q[0,action] = reward + dis * action_V\n",
    "            \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        \n",
    "    for w in w_batch:\n",
    "        w_stack = np.vstack([w])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack, w_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    \n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "    \n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN) :\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True :\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s,reward,done,_ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done :\n",
    "            print (\"Total score : {}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    max_episodes = 5000\n",
    "    end_episode = 0\n",
    "    step_count_total = 0\n",
    "    beta = beta_init\n",
    "    \n",
    "    replay_buffer = deque()\n",
    "    TD_error_list = []\n",
    "    steps_list = []\n",
    "    step_avg_list = []\n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name = \"target\",\n",
    "                                                src_scope_name = \"main\")\n",
    "        sess.run(copy_ops)\n",
    "    \n",
    "        for episode in range(1, max_episodes):\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            TD_error = 0\n",
    "            state = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                action_mat = mainDQN.predict(state)\n",
    "                action_max = softmax(action_mat[0])\n",
    "                action_max = np.cumsum(action_max)\n",
    "                \n",
    "                rand_batch = random.random()\n",
    "                AC_index = np.nonzero(action_max >= rand_batch)[0][0]\n",
    "                action = dec_action(AC_index)        \n",
    "                \n",
    "                next_state, reward, done, _ = env.step([action])\n",
    "                step_count += reward\n",
    "                \n",
    "                if done:\n",
    "                    #if step_count < 200:\n",
    "                    #    reward = -100\n",
    "                    TD_error = reward\n",
    "                else:\n",
    "                    action_mat = targetDQN.predict(next_state)\n",
    "                    action_V = softV(action_mat[0])\n",
    "                    TD_error = reward + dis * action_V\n",
    "                    \n",
    "                TD_error -= np.max(mainDQN.predict(state))\n",
    "                TD_error = pow((abs(TD_error) + eps), alpha)\n",
    "                TD_error_list.append(TD_error)\n",
    "                \n",
    "                if beta < 1:\n",
    "                    beta +=(1 - beta_init)/REPLAY_MEMORY\n",
    "                \n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                    replay_buffer.popleft()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "            print(\"episode: {}   steps: {}\".format(episode, step_count))\n",
    "            steps_list.append(step_count)\n",
    "            \n",
    "            if episode < 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / episode)\n",
    "                \n",
    "            if episode == 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "                \n",
    "            if episode > 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_count_total -= steps_list[episode - 101]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "            \n",
    "            sample = 0\n",
    "            if len(replay_buffer) < batch_size:\n",
    "                sample = len(replay_buffer)\n",
    "            else:\n",
    "                sample = batch_size\n",
    "            \n",
    "            TD_copy = []\n",
    "            TD_norm_list = []\n",
    "            TD_accum_list = []\n",
    "            W_is_list = []\n",
    "                \n",
    "            start = 0\n",
    "            len_TD = len(TD_error_list)\n",
    "            if(len_TD > REPLAY_MEMORY):\n",
    "                start = len_TD - REPLAY_MEMORY\n",
    "                TD_copy = TD_error_list[start : len_TD]\n",
    "                len_TD = REPLAY_MEMORY\n",
    "            else:\n",
    "                TD_copy = TD_error_list[:]\n",
    "                \n",
    "            sum_TD = sum(TD_copy)\n",
    "            TD_norm_list = [TD_copy[i] / sum_TD for i in range(len_TD)]\n",
    "            TD_accum_list = np.cumsum(TD_norm_list)\n",
    "                \n",
    "            #W_is_list = [np.power((REPLAY_MEMORY * TD_norm_list[i]), -beta) for i in range(len_TD)]\n",
    "            #maxW = np.max(W_is_list)\n",
    "            #W_is_list = [W_is_list[i] / maxW for i in range(len_TD)]\n",
    "                \n",
    "            W_is_list = np.ones([len(TD_accum_list)])\n",
    "                              \n",
    "            minibatch = []\n",
    "            w_batch = []\n",
    "                \n",
    "            TDT = np.zeros([len(TD_accum_list)])\n",
    "            for i in range(sample):\n",
    "                check = True\n",
    "                while check:\n",
    "                    rand_batch = random.random()\n",
    "                    TD_index = np.nonzero(TD_accum_list >= rand_batch)[0][0]\n",
    "                    if TDT[TD_index] == 0:\n",
    "                        TDT[TD_index] = 1\n",
    "                        check = False\n",
    "                    \n",
    "                w_batch.append(W_is_list[TD_index])\n",
    "                minibatch.append(replay_buffer[TD_index])\n",
    "                    \n",
    "            loss, _ = replay_train(mainDQN, targetDQN, minibatch, w_batch)\n",
    "                \n",
    "            #print (\"Loss :  \", loss)\n",
    "            sess.run(copy_ops)\n",
    "                \n",
    "            print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "            end_episode += 1\n",
    "            if step_avg_list[episode - 1] > 195:\n",
    "                break\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        for episode in range(end_episode + 1, max_episodes):\n",
    "            s = env.reset()\n",
    "            reward_sum = 0\n",
    "            while True :\n",
    "                #env.render()\n",
    "                a = np.argmax(mainDQN.predict(s))\n",
    "                s,reward,done,_ = env.step(a)\n",
    "                reward_sum += reward\n",
    "        \n",
    "                if done :\n",
    "                    #print(\"episode: {}   steps: {}\".format(episode, reward_sum))\n",
    "                    steps_list.append(reward_sum)\n",
    "                    step_count_total += steps_list[episode - 1]\n",
    "                    step_count_total -= steps_list[episode - 101]\n",
    "                    step_avg_list.append(step_count_total / 100)\n",
    "                    print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "                    break\n",
    "        \n",
    "        x_values = list(range(1, max_episodes))\n",
    "        y_values = step_avg_list[:]\n",
    "        plt.plot(x_values, y_values, c='green')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-09 02:12:51,900] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1   steps: -957.776438553\n",
      "1           -957.776438553\n",
      "episode: 2   steps: -1006.35928089\n",
      "2           -982.067859723\n",
      "episode: 3   steps: -1586.42510377\n",
      "3           -1183.5202744\n",
      "episode: 4   steps: -1654.82377789\n",
      "4           -1301.34615028\n",
      "episode: 5   steps: -1350.69397943\n",
      "5           -1311.21571611\n",
      "episode: 6   steps: -1502.9998376\n",
      "6           -1343.17973636\n",
      "episode: 7   steps: -1301.41972172\n",
      "7           -1337.21401998\n",
      "episode: 8   steps: -1686.14067334\n",
      "8           -1380.82985165\n",
      "episode: 9   steps: -1135.94851345\n",
      "9           -1353.62081407\n",
      "episode: 10   steps: -1101.73227983\n",
      "10           -1328.43196065\n",
      "episode: 11   steps: -1465.5747279\n",
      "11           -1340.89948494\n",
      "episode: 12   steps: -1169.91784559\n",
      "12           -1326.651015\n",
      "episode: 13   steps: -1321.55110407\n",
      "13           -1326.25871416\n",
      "episode: 14   steps: -1652.02993006\n",
      "14           -1349.52808672\n",
      "episode: 15   steps: -1167.13322859\n",
      "15           -1337.36842951\n",
      "episode: 16   steps: -1315.35632834\n",
      "16           -1335.99267319\n",
      "episode: 17   steps: -1479.5052231\n",
      "17           -1344.43458789\n",
      "episode: 18   steps: -1210.09183526\n",
      "18           -1336.97110163\n",
      "episode: 19   steps: -1098.20408667\n",
      "19           -1324.40441663\n",
      "episode: 20   steps: -1183.16228232\n",
      "20           -1317.34230992\n",
      "episode: 21   steps: -1090.22726956\n",
      "21           -1306.527308\n",
      "episode: 22   steps: -1171.07906425\n",
      "22           -1300.37056964\n",
      "episode: 23   steps: -976.684768978\n",
      "23           -1286.29727396\n",
      "episode: 24   steps: -1155.35360525\n",
      "24           -1280.84128777\n",
      "episode: 25   steps: -1187.30287853\n",
      "25           -1277.0997514\n",
      "episode: 26   steps: -950.126594495\n",
      "26           -1264.52386075\n",
      "episode: 27   steps: -1314.27836517\n",
      "27           -1266.36662017\n",
      "episode: 28   steps: -1295.66464298\n",
      "28           -1267.41297813\n",
      "episode: 29   steps: -966.230217034\n",
      "29           -1257.02736568\n",
      "episode: 30   steps: -1168.37790904\n",
      "30           -1254.07238379\n",
      "episode: 31   steps: -1780.82452723\n",
      "31           -1271.06438842\n",
      "episode: 32   steps: -1625.73645637\n",
      "32           -1282.14789054\n",
      "episode: 33   steps: -1448.59532968\n",
      "33           -1287.19175233\n",
      "episode: 34   steps: -1287.48817163\n",
      "34           -1287.20047055\n",
      "episode: 35   steps: -1594.18494872\n",
      "35           -1295.97145564\n",
      "episode: 36   steps: -1429.39342556\n",
      "36           -1299.67762147\n",
      "episode: 37   steps: -1728.530455\n",
      "37           -1311.26823859\n",
      "episode: 38   steps: -1120.32792142\n",
      "38           -1306.2434934\n",
      "episode: 39   steps: -1635.88349928\n",
      "39           -1314.69580125\n",
      "episode: 40   steps: -1260.54777879\n",
      "40           -1313.34210068\n",
      "episode: 41   steps: -1022.57404784\n",
      "41           -1306.25019696\n",
      "episode: 42   steps: -1511.09769539\n",
      "42           -1311.12751835\n",
      "episode: 43   steps: -1037.01716433\n",
      "43           -1304.75285895\n",
      "episode: 44   steps: -1784.36758562\n",
      "44           -1315.65319365\n",
      "episode: 45   steps: -1748.6486589\n",
      "45           -1325.2753151\n",
      "episode: 46   steps: -1697.92600723\n",
      "46           -1333.3764171\n",
      "episode: 47   steps: -1290.30843617\n",
      "47           -1332.46007708\n",
      "episode: 48   steps: -1077.93232875\n",
      "48           -1327.15741566\n",
      "episode: 49   steps: -1360.41707338\n",
      "49           -1327.83618418\n",
      "episode: 50   steps: -892.866076838\n",
      "50           -1319.13678204\n",
      "episode: 51   steps: -1047.06551399\n",
      "51           -1313.80205129\n",
      "episode: 52   steps: -1737.51656874\n",
      "52           -1321.95040739\n",
      "episode: 53   steps: -983.84728174\n",
      "53           -1315.57110314\n",
      "episode: 54   steps: -952.64675657\n",
      "54           -1308.8502819\n",
      "episode: 55   steps: -1027.5599172\n",
      "55           -1303.73591164\n",
      "episode: 56   steps: -1557.64589609\n",
      "56           -1308.2700185\n",
      "episode: 57   steps: -1038.70263583\n",
      "57           -1303.54076617\n",
      "episode: 58   steps: -1188.32773166\n",
      "58           -1301.55433455\n",
      "episode: 59   steps: -1051.06918731\n",
      "59           -1297.30882358\n",
      "episode: 60   steps: -1107.88339808\n",
      "60           -1294.15173315\n",
      "episode: 61   steps: -963.280816717\n",
      "61           -1288.72761977\n",
      "episode: 62   steps: -1250.55174\n",
      "62           -1288.11187977\n",
      "episode: 63   steps: -1634.10726334\n",
      "63           -1293.60386999\n",
      "episode: 64   steps: -1061.23947288\n",
      "64           -1289.97317628\n",
      "episode: 65   steps: -872.881400619\n",
      "65           -1283.55637973\n",
      "episode: 66   steps: -1682.39335642\n",
      "66           -1289.59936423\n",
      "episode: 67   steps: -1426.16151371\n",
      "67           -1291.63760526\n",
      "episode: 68   steps: -1077.12511338\n",
      "68           -1288.4830098\n",
      "episode: 69   steps: -1615.07852645\n",
      "69           -1293.21627815\n",
      "episode: 70   steps: -969.071439828\n",
      "70           -1288.5856376\n",
      "episode: 71   steps: -1012.85129029\n",
      "71           -1284.70205525\n",
      "episode: 72   steps: -1775.6018865\n",
      "72           -1291.52010846\n",
      "episode: 73   steps: -1495.19779132\n",
      "73           -1294.3102137\n",
      "episode: 74   steps: -1023.22698654\n",
      "74           -1290.64692685\n",
      "episode: 75   steps: -956.554212827\n",
      "75           -1286.19235733\n",
      "episode: 76   steps: -968.364419685\n",
      "76           -1282.01041078\n",
      "episode: 77   steps: -1739.92878593\n",
      "77           -1287.95740267\n",
      "episode: 78   steps: -1303.48794474\n",
      "78           -1288.15651218\n",
      "episode: 79   steps: -1181.4123782\n",
      "79           -1286.80532061\n",
      "episode: 80   steps: -1409.63888625\n",
      "80           -1288.34074018\n",
      "episode: 81   steps: -1142.10619263\n",
      "81           -1286.5353754\n",
      "episode: 82   steps: -983.69480397\n",
      "82           -1282.8421977\n",
      "episode: 83   steps: -1547.22589751\n",
      "83           -1286.02754348\n",
      "episode: 84   steps: -1867.95900586\n",
      "84           -1292.95529898\n",
      "episode: 85   steps: -1106.89735403\n",
      "85           -1290.76638198\n",
      "episode: 86   steps: -1697.66459768\n",
      "86           -1295.49775658\n",
      "episode: 87   steps: -1018.52333943\n",
      "87           -1292.31414259\n",
      "episode: 88   steps: -1075.34201758\n",
      "88           -1289.84855027\n",
      "episode: 89   steps: -1266.89933717\n",
      "89           -1289.59069394\n",
      "episode: 90   steps: -1039.22418903\n",
      "90           -1286.80884388\n",
      "episode: 91   steps: -1166.87267386\n",
      "91           -1285.49086399\n",
      "episode: 92   steps: -1834.78914214\n",
      "92           -1291.46149745\n",
      "episode: 93   steps: -1559.67870121\n",
      "93           -1294.34555341\n",
      "episode: 94   steps: -1285.26482841\n",
      "94           -1294.24894995\n",
      "episode: 95   steps: -1547.55110371\n",
      "95           -1296.91528841\n",
      "episode: 96   steps: -1250.1117975\n",
      "96           -1296.42775205\n",
      "episode: 97   steps: -1301.65362759\n",
      "97           -1296.48162705\n",
      "episode: 98   steps: -1021.83524485\n",
      "98           -1293.67911295\n",
      "episode: 99   steps: -1613.94817393\n",
      "99           -1296.91415397\n",
      "episode: 100   steps: -1795.01362777\n",
      "100           -1301.8951487\n",
      "episode: 101   steps: -1217.2601382\n",
      "101           -1304.4899857\n",
      "episode: 102   steps: -1030.25834411\n",
      "102           -1304.72897633\n",
      "episode: 103   steps: -1585.57142707\n",
      "103           -1304.72043957\n",
      "episode: 104   steps: -1117.38142261\n",
      "104           -1299.34601601\n",
      "episode: 105   steps: -1167.37557057\n",
      "105           -1297.51283193\n",
      "episode: 106   steps: -1040.43552978\n",
      "106           -1292.88718885\n",
      "episode: 107   steps: -998.367210607\n",
      "107           -1289.85666374\n",
      "episode: 108   steps: -1292.22204628\n",
      "108           -1285.91747747\n",
      "episode: 109   steps: -1297.40908981\n",
      "109           -1287.53208323\n",
      "episode: 110   steps: -1050.51677075\n",
      "110           -1287.01992814\n",
      "episode: 111   steps: -1484.587341\n",
      "111           -1287.21005427\n",
      "episode: 112   steps: -1061.65583311\n",
      "112           -1286.12743414\n",
      "episode: 113   steps: -1103.36656673\n",
      "113           -1283.94558877\n",
      "episode: 114   steps: -1064.13166584\n",
      "114           -1278.06660613\n",
      "episode: 115   steps: -1463.10941215\n",
      "115           -1281.02636796\n",
      "episode: 116   steps: -1184.63449816\n",
      "116           -1279.71914966\n",
      "episode: 117   steps: -1142.00228558\n",
      "117           -1276.34412029\n",
      "episode: 118   steps: -1330.09201734\n",
      "118           -1277.54412211\n",
      "episode: 119   steps: -1777.48414145\n",
      "119           -1284.33692266\n",
      "episode: 120   steps: -1009.25994016\n",
      "120           -1282.59789923\n",
      "episode: 121   steps: -1431.87202361\n",
      "121           -1286.01434677\n",
      "episode: 122   steps: -1067.02187771\n",
      "122           -1284.97377491\n",
      "episode: 123   steps: -1270.89235951\n",
      "123           -1287.91585081\n",
      "episode: 124   steps: -1143.70718526\n",
      "124           -1287.79938661\n",
      "episode: 125   steps: -1607.01938432\n",
      "125           -1291.99655167\n",
      "episode: 126   steps: -1299.25485972\n",
      "126           -1295.48783432\n",
      "episode: 127   steps: -1028.13975748\n",
      "127           -1292.62644825\n",
      "episode: 128   steps: -1337.08281491\n",
      "128           -1293.04062997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 129   steps: -1614.84478279\n",
      "129           -1299.52677562\n",
      "episode: 130   steps: -1123.32586206\n",
      "130           -1299.07625516\n",
      "episode: 131   steps: -1113.11368128\n",
      "131           -1292.3991467\n",
      "episode: 132   steps: -1300.56018164\n",
      "132           -1289.14738395\n",
      "episode: 133   steps: -1505.54772368\n",
      "133           -1289.71690789\n",
      "episode: 134   steps: -1468.13683743\n",
      "134           -1291.52339455\n",
      "episode: 135   steps: -1268.08684014\n",
      "135           -1288.26241346\n",
      "episode: 136   steps: -1653.20731676\n",
      "136           -1290.50055237\n",
      "episode: 137   steps: -1166.96475795\n",
      "137           -1284.8848954\n",
      "episode: 138   steps: -1076.48482739\n",
      "138           -1284.44646446\n",
      "episode: 139   steps: -1589.9268048\n",
      "139           -1283.98689752\n",
      "episode: 140   steps: -1345.04537374\n",
      "140           -1284.83187347\n",
      "episode: 141   steps: -1030.7798563\n",
      "141           -1284.91393155\n",
      "episode: 142   steps: -1518.22135198\n",
      "142           -1284.98516812\n",
      "episode: 143   steps: -1777.08602959\n",
      "143           -1292.38585677\n",
      "episode: 144   steps: -1633.80438511\n",
      "144           -1290.88022476\n",
      "episode: 145   steps: -1181.65178686\n",
      "145           -1285.21025604\n",
      "episode: 146   steps: -1721.3392256\n",
      "146           -1285.44438823\n",
      "episode: 147   steps: -1215.04656911\n",
      "147           -1284.69176956\n",
      "episode: 148   steps: -1664.73682977\n",
      "148           -1290.55981457\n",
      "episode: 149   steps: -1305.35784106\n",
      "149           -1290.00922224\n",
      "episode: 150   steps: -1437.84334665\n",
      "150           -1295.45899494\n",
      "episode: 151   steps: -1192.31341891\n",
      "151           -1296.91147399\n",
      "episode: 152   steps: -1073.07920505\n",
      "152           -1290.26710035\n",
      "episode: 153   steps: -1635.35488938\n",
      "153           -1296.78217643\n",
      "episode: 154   steps: -1289.31077181\n",
      "154           -1300.14881658\n",
      "episode: 155   steps: -1648.5346053\n",
      "155           -1306.35856346\n",
      "episode: 156   steps: -1053.36835759\n",
      "156           -1301.31578808\n",
      "episode: 157   steps: -1170.06843719\n",
      "157           -1302.62944609\n",
      "episode: 158   steps: -1357.32978632\n",
      "158           -1304.31946664\n",
      "episode: 159   steps: -1145.33797689\n",
      "159           -1305.26215454\n",
      "episode: 160   steps: -1202.02765243\n",
      "160           -1306.20359708\n",
      "episode: 161   steps: -1066.18633384\n",
      "161           -1307.23265225\n",
      "episode: 162   steps: -1150.88817538\n",
      "162           -1306.2360166\n",
      "episode: 163   steps: -1183.62403401\n",
      "163           -1301.73118431\n",
      "episode: 164   steps: -1172.89572707\n",
      "164           -1302.84774685\n",
      "episode: 165   steps: -1066.66603478\n",
      "165           -1304.78559319\n",
      "episode: 166   steps: -1194.62868974\n",
      "166           -1299.90794653\n",
      "episode: 167   steps: -1223.33938521\n",
      "167           -1297.87972524\n",
      "episode: 168   steps: -1274.08913818\n",
      "168           -1299.84936549\n",
      "episode: 169   steps: -1401.06457629\n",
      "169           -1297.70922599\n",
      "episode: 170   steps: -1298.99525717\n",
      "170           -1301.00846416\n",
      "episode: 171   steps: -1655.28394966\n",
      "171           -1307.43279076\n",
      "episode: 172   steps: -1500.58637178\n",
      "172           -1304.68263561\n",
      "episode: 173   steps: -1422.89085189\n",
      "173           -1303.95956621\n",
      "episode: 174   steps: -1265.61914787\n",
      "174           -1306.38348783\n",
      "episode: 175   steps: -1204.29574381\n",
      "175           -1308.86090314\n",
      "episode: 176   steps: -1801.54432612\n",
      "176           -1317.1927022\n",
      "episode: 177   steps: -1150.7796448\n",
      "177           -1311.30121079\n",
      "episode: 178   steps: -1171.71535278\n",
      "178           -1309.98348487\n",
      "episode: 179   steps: -1103.59632618\n",
      "179           -1309.20532435\n",
      "episode: 180   steps: -1194.34370073\n",
      "180           -1307.0523725\n",
      "episode: 181   steps: -1181.40947114\n",
      "181           -1307.44540528\n",
      "episode: 182   steps: -1120.37552414\n",
      "182           -1308.81221248\n",
      "episode: 183   steps: -1613.06220338\n",
      "183           -1309.47057554\n",
      "episode: 184   steps: -1186.77940103\n",
      "184           -1302.65877949\n",
      "episode: 185   steps: -1135.65529958\n",
      "185           -1302.94635895\n",
      "episode: 186   steps: -1139.80543202\n",
      "186           -1297.36776729\n",
      "episode: 187   steps: -1550.69132765\n",
      "187           -1302.68944717\n",
      "episode: 188   steps: -1105.03297006\n",
      "188           -1302.9863567\n",
      "episode: 189   steps: -1076.13934924\n",
      "189           -1301.07875682\n",
      "episode: 190   steps: -1190.54249754\n",
      "190           -1302.5919399\n",
      "episode: 191   steps: -1217.94787903\n",
      "191           -1303.10269196\n",
      "episode: 192   steps: -1082.82219439\n",
      "192           -1295.58302248\n",
      "episode: 193   steps: -1318.62753634\n",
      "193           -1293.17251083\n",
      "episode: 194   steps: -1609.02721659\n",
      "194           -1296.41013471\n",
      "episode: 195   steps: -1102.37166696\n",
      "195           -1291.95834034\n",
      "episode: 196   steps: -1223.36626921\n",
      "196           -1291.69088506\n",
      "episode: 197   steps: -1217.89434962\n",
      "197           -1290.85329228\n",
      "episode: 198   steps: -1644.61038076\n",
      "198           -1297.08104364\n",
      "episode: 199   steps: -1235.15976559\n",
      "199           -1293.29315956\n",
      "episode: 200   steps: -1669.86666366\n",
      "200           -1292.04168992\n",
      "episode: 201   steps: -1168.15795933\n",
      "201           -1291.55066813\n",
      "episode: 202   steps: -1592.10820498\n",
      "202           -1297.16916674\n",
      "episode: 203   steps: -1635.58125455\n",
      "203           -1297.66926501\n",
      "episode: 204   steps: -1182.97600117\n",
      "204           -1298.3252108\n",
      "episode: 205   steps: -1092.87007663\n",
      "205           -1297.58015586\n",
      "episode: 206   steps: -1280.68529371\n",
      "206           -1299.9826535\n",
      "episode: 207   steps: -1191.09080081\n",
      "207           -1301.9098894\n",
      "episode: 208   steps: -1225.37373529\n",
      "208           -1301.24140629\n",
      "episode: 209   steps: -1214.68913705\n",
      "209           -1300.41420676\n",
      "episode: 210   steps: -1367.1935723\n",
      "210           -1303.58097478\n",
      "episode: 211   steps: -1702.08129712\n",
      "211           -1305.75591434\n",
      "episode: 212   steps: -1556.64960225\n",
      "212           -1310.70585203\n",
      "episode: 213   steps: -1183.12321069\n",
      "213           -1311.50341847\n",
      "episode: 214   steps: -1068.90223994\n",
      "214           -1311.55112421\n",
      "episode: 215   steps: -1385.93217915\n",
      "215           -1310.77935188\n",
      "episode: 216   steps: -1177.08051654\n",
      "216           -1310.70381206\n",
      "episode: 217   steps: -1196.51665955\n",
      "217           -1311.2489558\n",
      "episode: 218   steps: -1155.05697222\n",
      "218           -1309.49860535\n",
      "episode: 219   steps: -1175.53223747\n",
      "219           -1303.47908631\n",
      "episode: 220   steps: -1186.10129332\n",
      "220           -1305.24749984\n",
      "episode: 221   steps: -1103.72465437\n",
      "221           -1301.96602615\n",
      "episode: 222   steps: -1779.05702928\n",
      "222           -1309.08637767\n",
      "episode: 223   steps: -1266.28954563\n",
      "223           -1309.04034953\n",
      "episode: 224   steps: -1189.66424743\n",
      "224           -1309.49992015\n",
      "episode: 225   steps: -1269.12253903\n",
      "225           -1306.1209517\n",
      "episode: 226   steps: -1422.00248521\n",
      "226           -1307.34842795\n",
      "episode: 227   steps: -1287.68740616\n",
      "227           -1309.94390444\n",
      "episode: 228   steps: -1242.84068942\n",
      "228           -1309.00148318\n",
      "episode: 229   steps: -1300.55056146\n",
      "229           -1305.85854097\n",
      "episode: 230   steps: -1303.94810412\n",
      "230           -1307.66476339\n",
      "episode: 231   steps: -1810.3953317\n",
      "231           -1314.6375799\n",
      "episode: 232   steps: -1246.74453453\n",
      "232           -1314.09942342\n",
      "episode: 233   steps: -1181.9102476\n",
      "233           -1310.86304866\n",
      "episode: 234   steps: -1127.94254676\n",
      "234           -1307.46110576\n",
      "episode: 235   steps: -1529.85680882\n",
      "235           -1310.07880544\n",
      "episode: 236   steps: -1036.38518991\n",
      "236           -1303.91058417\n",
      "episode: 237   steps: -1187.72949444\n",
      "237           -1304.11823154\n",
      "episode: 238   steps: -1871.51909759\n",
      "238           -1312.06857424\n",
      "episode: 239   steps: -1374.85551936\n",
      "239           -1309.91786139\n",
      "episode: 240   steps: -1337.17373947\n",
      "240           -1309.83914504\n",
      "episode: 241   steps: -1213.06676825\n",
      "241           -1311.66201416\n",
      "episode: 242   steps: -1656.60432265\n",
      "242           -1313.04584387\n",
      "episode: 243   steps: -1011.22765139\n",
      "243           -1305.38726009\n",
      "episode: 244   steps: -1605.91193984\n",
      "244           -1305.10833564\n",
      "episode: 245   steps: -1190.21257683\n",
      "245           -1305.19394354\n",
      "episode: 246   steps: -1246.97466048\n",
      "246           -1300.45029788\n",
      "episode: 247   steps: -1199.9806085\n",
      "247           -1300.29963828\n",
      "episode: 248   steps: -1213.84382665\n",
      "248           -1295.79070825\n",
      "episode: 249   steps: -1222.89219778\n",
      "249           -1294.96605181\n",
      "episode: 250   steps: -1167.31972016\n",
      "250           -1292.26081555\n",
      "episode: 251   steps: -1228.54380779\n",
      "251           -1292.62311944\n",
      "episode: 252   steps: -1171.28695816\n",
      "252           -1293.60519697\n",
      "episode: 253   steps: -1428.17804996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "253           -1291.53342858\n",
      "episode: 254   steps: -1260.39030823\n",
      "254           -1291.24422394\n",
      "episode: 255   steps: -1149.06473929\n",
      "255           -1286.24952528\n",
      "episode: 256   steps: -1726.30440646\n",
      "256           -1292.97888577\n",
      "episode: 257   steps: -1104.49482479\n",
      "257           -1292.32314964\n",
      "episode: 258   steps: -1189.61852221\n",
      "258           -1290.646037\n",
      "episode: 259   steps: -1168.50746582\n",
      "259           -1290.87773189\n",
      "episode: 260   steps: -1006.74531289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-c6d922b29450>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTD_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0;31m#print (\"Loss :  \", loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-3673c6aa0157>\u001b[0m in \u001b[0;36mreplay_train\u001b[0;34m(mainDQN, targetDQN, train_batch, w_batch)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0maction_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargetDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0maction_V\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdis\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction_V\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e8ee6ff44527>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Qpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1291\u001b[0m                 run_metadata):\n\u001b[1;32m   1292\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1338\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1339\u001b[0m     \u001b[0;31m# Nothing to do if we're using the new session interface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m     \u001b[0;31m# TODO(skyewm): remove this function altogether eventually\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
