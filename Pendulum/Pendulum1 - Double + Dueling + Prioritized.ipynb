{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque\n",
    "import gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-09 03:20:53,741] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')\n",
    "\n",
    "num_actions = 16\n",
    "min_act = -2\n",
    "max_act = 2\n",
    "dis = 0.99\n",
    "REPLAY_MEMORY = 10000\n",
    "batch_size = 256\n",
    "alpha = 0.6\n",
    "beta_init = 0.4\n",
    "eps = 0.01\n",
    "\n",
    "input_size = env.observation_space.shape[0]\n",
    "output_size = num_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN :\n",
    "    def __init__(self, session, input_size, output_size, name=\"main\") :\n",
    "        self.session = session\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.net_name = name\n",
    "        self._build_network()\n",
    "        \n",
    "    def _build_network(self, h_size=64, l_rate=0.01) :\n",
    "        with tf.variable_scope(self.net_name):\n",
    "            self._X = tf.placeholder(tf.float32, [None, self.input_size], name=\"input_x\")\n",
    "            \n",
    "            W1 = tf.get_variable(\"W1\", shape=[self.input_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer1 = tf.nn.relu(tf.matmul(self._X, W1))\n",
    "            \n",
    "            W2 = tf.get_variable(\"W2\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            W3 = tf.get_variable(\"W3\", shape=[h_size, h_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            layer2 = tf.nn.relu(tf.matmul(layer1, W2))\n",
    "            layer3 = tf.nn.relu(tf.matmul(layer1, W3))\n",
    "            \n",
    "            W_V = tf.get_variable(\"W_V\", shape=[h_size, 1],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            W_A = tf.get_variable(\"W_A\", shape=[h_size, self.output_size],\n",
    "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
    "            \n",
    "            self.Value = tf.matmul(layer2, W_V)\n",
    "            self.Advantage = tf.matmul(layer3, W_A)\n",
    "            \n",
    "            self._Qpred = self.Value + self.Advantage - tf.reduce_mean(self.Advantage,\n",
    "                                                                       reduction_indices=1,keep_dims=True)\n",
    "        \n",
    "        self._Y = tf.placeholder(shape=[None, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._WIS = tf.placeholder(shape=[1, 1], dtype=tf.float32)\n",
    "        #self._WIS = tf.placeholder(shape=[1, self.output_size], dtype=tf.float32)\n",
    "        \n",
    "        self._loss = tf.reduce_mean(tf.square(self._Y - self._Qpred))\n",
    "        #self._loss = tf.reduce_mean(tf.multiply(self._WIS, tf.square(self._Y - self._Qpred)))\n",
    "        #self._loss = tf.reduce_mean(self._WIS * tf.square(self._Y - self._Qpred))\n",
    "        \n",
    "        self._train = tf.train.AdamOptimizer(learning_rate = l_rate).minimize(self._loss)\n",
    "    \n",
    "    def predict(self, state):\n",
    "        x = np.reshape(state, [1,self.input_size])\n",
    "        return self.session.run(self._Qpred, feed_dict={self._X : x})\n",
    "    \n",
    "    def update(self, x_stack, y_stack, w_stack):\n",
    "        return self.session.run([self._loss, self._train],\n",
    "                                feed_dict={self._X : x_stack, self._Y : y_stack, self._WIS : w_stack})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replay_train (mainDQN, targetDQN, train_batch, w_batch) :\n",
    "    x_stack = np.empty(0).reshape(0, input_size)\n",
    "    y_stack = np.empty(0).reshape(0, output_size)\n",
    "    w_stack = np.empty(0).reshape(0, 0)\n",
    "    \n",
    "    for state, action, reward, next_state, done in train_batch:\n",
    "        Q = mainDQN.predict(state)\n",
    "        \n",
    "        if done :\n",
    "            Q[0,action] = reward\n",
    "        else :\n",
    "            action0 = np.argmax(mainDQN.predict(next_state))\n",
    "            Q[0,action] = reward + dis * (targetDQN.predict(next_state)[0, action0])\n",
    "    \n",
    "        y_stack = np.vstack([y_stack, Q])\n",
    "        x_stack = np.vstack([x_stack, state])\n",
    "        \n",
    "    for w in w_batch:\n",
    "        w_stack = np.vstack([w])\n",
    "        \n",
    "    return mainDQN.update(x_stack, y_stack, w_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dec_action(a):\n",
    "    return min_act + a * (max_act - min_act) / (num_actions - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_var_ops(dest_scope_name=\"target\", src_scope_name=\"main\"):\n",
    "    op_holder = []\n",
    "    \n",
    "    src_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = src_scope_name)\n",
    "    \n",
    "    dest_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope = dest_scope_name)\n",
    "    \n",
    "    for src_var, dest_var in zip(src_vars, dest_vars):\n",
    "        op_holder.append(dest_var.assign(src_var.value()))\n",
    "    \n",
    "    return op_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bot_play(mainDQN) :\n",
    "    s = env.reset()\n",
    "    reward_sum = 0\n",
    "    while True :\n",
    "        env.render()\n",
    "        a = np.argmax(mainDQN.predict(s))\n",
    "        s,reward,done,_ = env.step(a)\n",
    "        reward_sum += reward\n",
    "        \n",
    "        if done :\n",
    "            print (\"Total score : {}\".format(reward_sum))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    max_episodes = 500\n",
    "    end_episode = 0\n",
    "    step_count_total = 0\n",
    "    beta = beta_init\n",
    "    \n",
    "    replay_buffer = deque()\n",
    "    TD_error_list = []\n",
    "    steps_list = []\n",
    "    step_avg_list = []\n",
    "    \n",
    "    with tf.Session() as sess :\n",
    "        mainDQN = DQN(sess, input_size, output_size, name=\"main\")\n",
    "        targetDQN = DQN(sess, input_size, output_size, name=\"target\")\n",
    "        \n",
    "        tf.initialize_all_variables().run()\n",
    "        copy_ops = get_copy_var_ops(dest_scope_name = \"target\",\n",
    "                                                src_scope_name = \"main\")\n",
    "        sess.run(copy_ops)\n",
    "    \n",
    "        for episode in range(1, max_episodes):\n",
    "            e = 1. / (((episode - 1) / 5) + 1)\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            TD_error = 0\n",
    "            state = env.reset()\n",
    "            \n",
    "            while not done:\n",
    "                if np.random.rand(1) < e:\n",
    "                    action = random.randrange(num_actions)\n",
    "                else:\n",
    "                    action = np.argmax(mainDQN.predict(state))\n",
    "                \n",
    "                action_1 = dec_action(action)        \n",
    "                \n",
    "                next_state, reward, done, _ = env.step([action_1])\n",
    "                step_count += reward\n",
    "                \n",
    "                if done:\n",
    "                    #if step_count < 200:\n",
    "                    #    reward = -100\n",
    "                    TD_error = reward\n",
    "                else:\n",
    "                    action0 = np.argmax(mainDQN.predict(next_state))\n",
    "                    TD_error = reward + dis * (targetDQN.predict(next_state)[0, action0])\n",
    "                    \n",
    "                TD_error -= np.max(mainDQN.predict(state))\n",
    "                TD_error = pow((abs(TD_error) + eps), alpha)\n",
    "                TD_error_list.append(TD_error)\n",
    "                \n",
    "                if beta < 1:\n",
    "                    beta +=(1 - beta_init)/REPLAY_MEMORY\n",
    "                \n",
    "                replay_buffer.append((state, action, reward, next_state, done))\n",
    "                if len(replay_buffer) > REPLAY_MEMORY:\n",
    "                    replay_buffer.popleft()\n",
    "                \n",
    "                state = next_state\n",
    "                \n",
    "                sample = 0\n",
    "                if len(replay_buffer) < batch_size:\n",
    "                    sample = len(replay_buffer)\n",
    "                else:\n",
    "                    sample = batch_size\n",
    "\n",
    "                TD_copy = []\n",
    "                TD_norm_list = []\n",
    "                TD_accum_list = []\n",
    "                W_is_list = []\n",
    "\n",
    "                start = 0\n",
    "                len_TD = len(TD_error_list)\n",
    "                if(len_TD > REPLAY_MEMORY):\n",
    "                    start = len_TD - REPLAY_MEMORY\n",
    "                    TD_copy = TD_error_list[start : len_TD]\n",
    "                    len_TD = REPLAY_MEMORY\n",
    "                else:\n",
    "                    TD_copy = TD_error_list[:]\n",
    "\n",
    "                sum_TD = sum(TD_copy)\n",
    "                TD_norm_list = [TD_copy[i] / sum_TD for i in range(len_TD)]\n",
    "                TD_accum_list = np.cumsum(TD_norm_list)\n",
    "                W_is_list = np.ones([len(TD_accum_list)])\n",
    "\n",
    "                minibatch = []\n",
    "                w_batch = []\n",
    "                TDT = np.zeros([len(TD_accum_list)])\n",
    "                for i in range(sample):\n",
    "                    check = True\n",
    "                    while check:\n",
    "                        rand_batch = random.random()\n",
    "                        TD_index = np.nonzero(TD_accum_list >= rand_batch)[0][0]\n",
    "                        if TDT[TD_index] == 0:\n",
    "                            TDT[TD_index] = 1\n",
    "                            check = False\n",
    "\n",
    "                    w_batch.append(W_is_list[TD_index])\n",
    "                    minibatch.append(replay_buffer[TD_index])\n",
    "\n",
    "                loss, _ = replay_train(mainDQN, targetDQN, minibatch, w_batch)\n",
    "\n",
    "                #print (\"Loss :  \", loss)\n",
    "                sess.run(copy_ops)\n",
    "                \n",
    "            print(\"episode: {}   steps: {}\".format(episode, step_count))\n",
    "            steps_list.append(step_count)\n",
    "            \n",
    "            if episode < 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / episode)\n",
    "                \n",
    "            if episode == 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "                \n",
    "            if episode > 100:\n",
    "                step_count_total += steps_list[episode - 1]\n",
    "                step_count_total -= steps_list[episode - 101]\n",
    "                step_avg_list.append(step_count_total / 100)\n",
    "                #print (\"Step Average 100:  \", step_avg_list[episode - 1])\n",
    "             \n",
    "            print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "            end_episode += 1\n",
    "            if step_avg_list[episode - 1] > 195:\n",
    "                break\n",
    "        \n",
    "        print(\"--------------------------------------------------\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "        for episode in range(end_episode + 1, max_episodes):\n",
    "            s = env.reset()\n",
    "            reward_sum = 0\n",
    "            while True :\n",
    "                #env.render()\n",
    "                a = np.argmax(mainDQN.predict(s))\n",
    "                s,reward,done,_ = env.step(a)\n",
    "                reward_sum += reward\n",
    "        \n",
    "                if done :\n",
    "                    #print(\"episode: {}   steps: {}\".format(episode, reward_sum))\n",
    "                    steps_list.append(reward_sum)\n",
    "                    step_count_total += steps_list[episode - 1]\n",
    "                    step_count_total -= steps_list[episode - 101]\n",
    "                    step_avg_list.append(step_count_total / 100)\n",
    "                    print(\"{}           {}\".format(episode, step_avg_list[episode - 1]))\n",
    "                    break\n",
    "        \n",
    "        x_values = list(range(1, max_episodes))\n",
    "        y_values = step_avg_list[:]\n",
    "        plt.plot(x_values, y_values, c='green')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2018-02-09 03:20:55,441] From /usr/local/lib/python2.7/dist-packages/tensorflow/python/util/tf_should_use.py:107: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 1   steps: -1675.56445292\n",
      "1           -1675.56445292\n",
      "episode: 2   steps: -1181.95429484\n",
      "2           -1428.75937388\n",
      "episode: 3   steps: -1731.03560191\n",
      "3           -1529.51811656\n",
      "episode: 4   steps: -1240.69784746\n",
      "4           -1457.31304928\n",
      "episode: 5   steps: -1024.75046692\n",
      "5           -1370.80053281\n",
      "episode: 6   steps: -1404.27443015\n",
      "6           -1376.3795157\n",
      "episode: 7   steps: -1312.4491193\n",
      "7           -1367.24660193\n",
      "episode: 8   steps: -1254.46543082\n",
      "8           -1353.14895554\n",
      "episode: 9   steps: -1623.96204066\n",
      "9           -1383.23929833\n",
      "episode: 10   steps: -1451.68372356\n",
      "10           -1390.08374085\n",
      "episode: 11   steps: -1369.91538574\n",
      "11           -1388.25025403\n",
      "episode: 12   steps: -1374.86145556\n",
      "12           -1387.13452082\n",
      "episode: 13   steps: -1388.34611994\n",
      "13           -1387.22772075\n",
      "episode: 14   steps: -1329.97654389\n",
      "14           -1383.13835098\n",
      "episode: 15   steps: -1425.4714365\n",
      "15           -1385.96055668\n",
      "episode: 16   steps: -1353.33420403\n",
      "16           -1383.92140964\n",
      "episode: 17   steps: -1344.87457144\n",
      "17           -1381.6245368\n",
      "episode: 18   steps: -1346.73535842\n",
      "18           -1379.68624911\n",
      "episode: 19   steps: -1615.27508924\n",
      "19           -1392.08566175\n",
      "episode: 20   steps: -1582.15674086\n",
      "20           -1401.58921571\n",
      "episode: 21   steps: -1485.90921457\n",
      "21           -1405.60445375\n",
      "episode: 22   steps: -1611.16578729\n",
      "22           -1414.94815073\n",
      "episode: 23   steps: -1764.12987767\n",
      "23           -1430.12996494\n",
      "episode: 24   steps: -1755.40466089\n",
      "24           -1443.68307727\n",
      "episode: 25   steps: -1750.38499511\n",
      "25           -1455.95115399\n",
      "episode: 26   steps: -1798.62300508\n",
      "26           -1469.13084057\n",
      "episode: 27   steps: -1808.64055524\n",
      "27           -1481.70527444\n",
      "episode: 28   steps: -1940.63162206\n",
      "28           -1498.09550115\n",
      "episode: 29   steps: -1921.25969492\n",
      "29           -1512.6873699\n",
      "episode: 30   steps: -1612.39288236\n",
      "30           -1516.01088698\n",
      "episode: 31   steps: -1618.95133958\n",
      "31           -1519.33154674\n",
      "episode: 32   steps: -1821.86235728\n",
      "32           -1528.78563457\n",
      "episode: 33   steps: -1721.08075892\n",
      "33           -1534.61275955\n",
      "episode: 34   steps: -1814.79642434\n",
      "34           -1542.85345557\n",
      "episode: 35   steps: -1917.28025252\n",
      "35           -1553.55136406\n",
      "episode: 36   steps: -1814.93996274\n",
      "36           -1560.81215846\n",
      "episode: 37   steps: -1865.15842817\n",
      "37           -1569.03773332\n",
      "episode: 38   steps: -1848.99062645\n",
      "38           -1576.40491472\n",
      "episode: 39   steps: -1626.55262727\n",
      "39           -1577.6907535\n",
      "episode: 40   steps: -1054.66235138\n",
      "40           -1564.61504345\n",
      "episode: 41   steps: -1453.90798842\n",
      "41           -1561.91487138\n",
      "episode: 42   steps: -1467.901134\n",
      "42           -1559.67644906\n",
      "episode: 43   steps: -1539.09107738\n",
      "43           -1559.19771948\n",
      "episode: 44   steps: -1559.45905042\n",
      "44           -1559.20365882\n",
      "episode: 45   steps: -1409.99559143\n",
      "45           -1555.88792399\n",
      "episode: 46   steps: -1299.61071926\n",
      "46           -1550.31668041\n",
      "episode: 47   steps: -1573.14487538\n",
      "47           -1550.80238669\n",
      "episode: 48   steps: -1537.46392319\n",
      "48           -1550.52450203\n",
      "episode: 49   steps: -1344.07898264\n",
      "49           -1546.31132817\n",
      "episode: 50   steps: -1516.44661788\n",
      "50           -1545.71403396\n",
      "episode: 51   steps: -1272.39975261\n",
      "51           -1540.3549304\n",
      "episode: 52   steps: -1360.27274994\n",
      "52           -1536.89181155\n",
      "episode: 53   steps: -1074.91710723\n",
      "53           -1528.17530769\n",
      "episode: 54   steps: -1288.02855474\n",
      "54           -1523.7281456\n",
      "episode: 55   steps: -1560.41459156\n",
      "55           -1524.39517189\n",
      "episode: 56   steps: -1835.43259355\n",
      "56           -1529.94941156\n",
      "episode: 57   steps: -1376.54541554\n",
      "57           -1527.25811339\n",
      "episode: 58   steps: -1461.55981232\n",
      "58           -1526.12538406\n",
      "episode: 59   steps: -1337.22575177\n",
      "59           -1522.92369538\n",
      "episode: 60   steps: -1400.93011951\n",
      "60           -1520.89046911\n",
      "episode: 61   steps: -1514.29680642\n",
      "61           -1520.78237628\n",
      "episode: 62   steps: -1108.25445933\n",
      "62           -1514.1287002\n",
      "episode: 63   steps: -1206.11272367\n",
      "63           -1509.23955772\n",
      "episode: 64   steps: -1209.52092119\n",
      "64           -1504.55645402\n",
      "episode: 65   steps: -1363.80609826\n",
      "65           -1502.39106393\n",
      "episode: 66   steps: -1323.29199901\n",
      "66           -1499.67744174\n",
      "episode: 67   steps: -1325.06103508\n",
      "67           -1497.07122671\n",
      "episode: 68   steps: -1295.02623806\n",
      "68           -1494.09997688\n",
      "episode: 69   steps: -1286.96257562\n",
      "69           -1491.09798556\n",
      "episode: 70   steps: -1220.89198132\n",
      "70           -1487.23789978\n",
      "episode: 71   steps: -1139.20027539\n",
      "71           -1482.33596141\n",
      "episode: 72   steps: -1160.39533286\n",
      "72           -1477.86456379\n",
      "episode: 73   steps: -1355.51252592\n",
      "73           -1476.18850848\n",
      "episode: 74   steps: -1097.60491912\n",
      "74           -1471.07251403\n",
      "episode: 75   steps: -1412.16508776\n",
      "75           -1470.28708168\n",
      "episode: 76   steps: -1108.68470051\n",
      "76           -1465.52915561\n",
      "episode: 77   steps: -1121.0355473\n",
      "77           -1461.05521264\n",
      "episode: 78   steps: -1126.90253965\n",
      "78           -1456.77120402\n",
      "episode: 79   steps: -1001.22479856\n",
      "79           -1451.00479382\n",
      "episode: 80   steps: -1371.64886908\n",
      "80           -1450.01284476\n",
      "episode: 81   steps: -1102.52142129\n",
      "81           -1445.72282719\n",
      "episode: 82   steps: -1059.29930108\n",
      "82           -1441.01034516\n",
      "episode: 83   steps: -1055.08479165\n",
      "83           -1436.3606397\n",
      "episode: 84   steps: -1489.88443408\n",
      "84           -1436.99782773\n",
      "episode: 85   steps: -1485.44553066\n",
      "85           -1437.5678007\n",
      "episode: 86   steps: -1189.07602785\n",
      "86           -1434.67836148\n",
      "episode: 87   steps: -1260.29175518\n",
      "87           -1432.67391773\n",
      "episode: 88   steps: -1364.48389323\n",
      "88           -1431.89903109\n",
      "episode: 89   steps: -1219.19106063\n",
      "89           -1429.50905389\n",
      "episode: 90   steps: -1372.36935633\n",
      "90           -1428.87416836\n",
      "episode: 91   steps: -1337.66426412\n",
      "91           -1427.87186172\n",
      "episode: 92   steps: -1184.09988462\n",
      "92           -1425.22216632\n",
      "episode: 93   steps: -1359.56190239\n",
      "93           -1424.51614198\n",
      "episode: 94   steps: -1259.23888326\n",
      "94           -1422.75787327\n",
      "episode: 95   steps: -1299.86521682\n",
      "95           -1421.46426636\n",
      "episode: 96   steps: -1299.94921323\n",
      "96           -1420.19848455\n",
      "episode: 97   steps: -1304.70067657\n",
      "97           -1419.0077855\n",
      "episode: 98   steps: -1284.24009507\n",
      "98           -1417.63260499\n",
      "episode: 99   steps: -1317.33111197\n",
      "99           -1416.61945859\n",
      "episode: 100   steps: -1318.03207189\n",
      "100           -1415.63358473\n",
      "episode: 101   steps: -1410.65598303\n",
      "101           -1412.98450003\n",
      "episode: 102   steps: -1373.13801379\n",
      "102           -1414.89633722\n",
      "episode: 103   steps: -1380.86028453\n",
      "103           -1411.39458404\n",
      "episode: 104   steps: -1399.70167686\n",
      "104           -1412.98462234\n",
      "episode: 105   steps: -1461.13834629\n",
      "105           -1417.34850113\n",
      "episode: 106   steps: -1405.83731604\n",
      "106           -1417.36412999\n",
      "episode: 107   steps: -1495.60335411\n",
      "107           -1419.19567234\n",
      "episode: 108   steps: -1454.56338179\n",
      "108           -1421.19665185\n",
      "episode: 109   steps: -1648.0430313\n",
      "109           -1421.43746176\n",
      "episode: 110   steps: -1150.6220812\n",
      "110           -1418.42684533\n",
      "episode: 111   steps: -1349.21779028\n",
      "111           -1418.21986938\n",
      "episode: 112   steps: -1225.24042876\n",
      "112           -1416.72365911\n",
      "episode: 113   steps: -1182.81371047\n",
      "113           -1414.66833501\n",
      "episode: 114   steps: -1495.78549063\n",
      "114           -1416.32642448\n",
      "episode: 115   steps: -1516.67925232\n",
      "115           -1417.23850264\n",
      "episode: 116   steps: -1381.64441893\n",
      "116           -1417.52160479\n",
      "episode: 117   steps: -1317.38060121\n",
      "117           -1417.24666509\n",
      "episode: 118   steps: -1530.8115144\n",
      "118           -1419.08742665\n",
      "episode: 119   steps: -1507.4318514\n",
      "119           -1418.00899427\n",
      "episode: 120   steps: -1441.10169946\n",
      "120           -1416.59844385\n",
      "episode: 121   steps: -1319.21317007\n",
      "121           -1414.93148341\n",
      "episode: 122   steps: -1284.95239161\n",
      "122           -1411.66934945\n",
      "episode: 123   steps: -1357.55145421\n",
      "123           -1407.60356522\n",
      "episode: 124   steps: -1479.20102152\n",
      "124           -1404.84152882\n",
      "episode: 125   steps: -1508.39727138\n",
      "125           -1402.42165159\n",
      "episode: 126   steps: -1517.423004\n",
      "126           -1399.60965158\n",
      "episode: 127   steps: -1514.55152287\n",
      "127           -1396.66876125\n",
      "episode: 128   steps: -1607.90427048\n",
      "128           -1393.34148774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 129   steps: -1521.16299949\n",
      "129           -1389.34052078\n",
      "episode: 130   steps: -1350.03560642\n",
      "130           -1386.71694802\n",
      "episode: 131   steps: -1316.65079751\n",
      "131           -1383.6939426\n",
      "episode: 132   steps: -919.831048009\n",
      "132           -1374.67362951\n",
      "episode: 133   steps: -1387.42033224\n",
      "133           -1371.33702524\n",
      "episode: 134   steps: -1348.38554578\n",
      "134           -1366.67291646\n",
      "episode: 135   steps: -1265.60317985\n",
      "135           -1360.15614573\n",
      "episode: 136   steps: -1428.00648045\n",
      "136           -1356.28681091\n",
      "episode: 137   steps: -1446.14260704\n",
      "137           -1352.0966527\n",
      "episode: 138   steps: -1360.24781188\n",
      "138           -1347.20922455\n",
      "episode: 139   steps: -1419.5061172\n",
      "139           -1345.13875945\n",
      "episode: 140   steps: -1337.78126534\n",
      "140           -1347.96994859\n",
      "episode: 141   steps: -1288.38566352\n",
      "141           -1346.31472534\n",
      "episode: 142   steps: -1310.99479639\n",
      "142           -1344.74566196\n",
      "episode: 143   steps: -1169.5831687\n",
      "143           -1341.05058288\n",
      "episode: 144   steps: -1321.50434547\n",
      "144           -1338.67103583\n",
      "episode: 145   steps: -1388.83279641\n",
      "145           -1338.45940788\n",
      "episode: 146   steps: -901.197526898\n",
      "146           -1334.47527595\n",
      "episode: 147   steps: -1400.37488908\n",
      "147           -1332.74757609\n",
      "episode: 148   steps: -1444.53043993\n",
      "148           -1331.81824126\n",
      "episode: 149   steps: -1363.26129871\n",
      "149           -1332.01006442\n",
      "episode: 150   steps: -1472.93407799\n",
      "150           -1331.57493902\n",
      "episode: 151   steps: -1122.66863497\n",
      "151           -1330.07762784\n",
      "episode: 152   steps: -1101.61778949\n",
      "152           -1327.49107824\n",
      "episode: 153   steps: -1122.34587013\n",
      "153           -1327.96536587\n",
      "episode: 154   steps: -1131.73752736\n",
      "154           -1326.40245559\n",
      "episode: 155   steps: -1369.62724923\n",
      "155           -1324.49458217\n",
      "episode: 156   steps: -1283.98219904\n",
      "156           -1318.98007823\n",
      "episode: 157   steps: -1171.35136368\n",
      "157           -1316.92813771\n",
      "episode: 158   steps: -1222.16906051\n",
      "158           -1314.53423019\n",
      "episode: 159   steps: -1486.77779767\n",
      "159           -1316.02975065\n",
      "episode: 160   steps: -1148.75314471\n",
      "160           -1313.5079809\n",
      "episode: 161   steps: -937.524208011\n",
      "161           -1307.74025492\n",
      "episode: 162   steps: -995.509805681\n",
      "162           -1306.61280838\n",
      "episode: 163   steps: -1659.31477941\n",
      "163           -1311.14482894\n",
      "episode: 164   steps: -1549.31218707\n",
      "164           -1314.5427416\n",
      "episode: 165   steps: -1555.55103589\n",
      "165           -1316.46019097\n",
      "episode: 166   steps: -1298.16172364\n",
      "166           -1316.20888822\n",
      "episode: 167   steps: -1298.00112551\n",
      "167           -1315.93828912\n",
      "episode: 168   steps: -1185.23914515\n",
      "168           -1314.84041819\n",
      "episode: 169   steps: -1476.76675559\n",
      "169           -1316.73845999\n",
      "episode: 170   steps: -1174.49447721\n",
      "170           -1316.27448495\n",
      "episode: 171   steps: -1178.54572481\n",
      "171           -1316.66793945\n",
      "episode: 172   steps: -1347.43976219\n",
      "172           -1318.53838374\n",
      "episode: 173   steps: -1177.14325639\n",
      "173           -1316.75469104\n",
      "episode: 174   steps: -883.29316649\n",
      "174           -1314.61157352\n",
      "episode: 175   steps: -1438.24943672\n",
      "175           -1314.87241701\n",
      "episode: 176   steps: -893.952098423\n",
      "176           -1312.72509099\n",
      "episode: 177   steps: -1343.88958859\n",
      "177           -1314.9536314\n",
      "episode: 178   steps: -1274.0378054\n",
      "178           -1316.42498406\n",
      "episode: 179   steps: -1377.09176658\n",
      "179           -1320.18365374\n",
      "episode: 180   steps: -1357.29022109\n",
      "180           -1320.04006726\n",
      "episode: 181   steps: -1152.39024833\n",
      "181           -1320.53875553\n",
      "episode: 182   steps: -1354.51364372\n",
      "182           -1323.49089896\n",
      "episode: 183   steps: -1491.81988824\n",
      "183           -1327.85824992\n",
      "episode: 184   steps: -1485.49906471\n",
      "184           -1327.81439623\n",
      "episode: 185   steps: -1484.94292336\n",
      "185           -1327.80937015\n",
      "episode: 186   steps: -1370.73815148\n",
      "186           -1329.62599139\n",
      "episode: 187   steps: -1416.57107525\n",
      "187           -1331.18878459\n",
      "episode: 188   steps: -1516.39743672\n",
      "188           -1332.70792003\n",
      "episode: 189   steps: -1380.84171772\n",
      "189           -1334.3244266\n",
      "episode: 190   steps: -963.347786979\n",
      "190           -1330.2342109\n",
      "episode: 191   steps: -1377.86001086\n",
      "191           -1330.63616837\n",
      "episode: 192   steps: -1440.33061903\n",
      "192           -1333.19847572\n",
      "episode: 193   steps: -1361.20458148\n",
      "193           -1333.21490251\n",
      "episode: 194   steps: -1206.46058046\n",
      "194           -1332.68711948\n",
      "episode: 195   steps: -1206.56701024\n",
      "195           -1331.75413741\n",
      "episode: 196   steps: -970.577523944\n",
      "196           -1328.46042052\n",
      "episode: 197   steps: -1171.08631161\n",
      "197           -1327.12427687\n",
      "episode: 198   steps: -1141.70178194\n",
      "198           -1325.69889374\n",
      "episode: 199   steps: -1057.10176162\n",
      "199           -1323.09660024\n",
      "episode: 200   steps: -992.150519992\n",
      "200           -1319.83778472\n",
      "episode: 201   steps: -1191.94599297\n",
      "201           -1317.65068482\n",
      "episode: 202   steps: -1209.71879569\n",
      "202           -1316.01649263\n",
      "episode: 203   steps: -1353.74369625\n",
      "203           -1315.74532675\n",
      "episode: 204   steps: -1222.86371816\n",
      "204           -1313.97694716\n",
      "episode: 205   steps: -1300.42264714\n",
      "205           -1312.36979017\n",
      "episode: 206   steps: -1325.18727511\n",
      "206           -1311.56328976\n",
      "episode: 207   steps: -1488.75422597\n",
      "207           -1311.49479848\n",
      "episode: 208   steps: -1500.1616868\n",
      "208           -1311.95078153\n",
      "episode: 209   steps: -1343.04262424\n",
      "209           -1308.90077746\n",
      "episode: 210   steps: -1334.22767552\n",
      "210           -1310.73683341\n",
      "episode: 211   steps: -1079.25538175\n",
      "211           -1308.03720932\n",
      "episode: 212   steps: -1196.07398316\n",
      "212           -1307.74554486\n",
      "episode: 213   steps: -1210.96864751\n",
      "213           -1308.02709423\n",
      "episode: 214   steps: -1105.20719312\n",
      "214           -1304.12131126\n",
      "episode: 215   steps: -1043.46093568\n",
      "215           -1299.38912809\n",
      "episode: 216   steps: -1154.22806962\n",
      "216           -1297.1149646\n",
      "episode: 217   steps: -1147.7308597\n",
      "217           -1295.41846718\n",
      "episode: 218   steps: -1190.92057593\n",
      "218           -1292.0195578\n",
      "episode: 219   steps: -1253.40201885\n",
      "219           -1289.47925947\n",
      "episode: 220   steps: -1183.22962423\n",
      "220           -1286.90053872\n",
      "episode: 221   steps: -1338.26514427\n",
      "221           -1287.09105846\n",
      "episode: 222   steps: -1068.50648027\n",
      "222           -1284.92659935\n",
      "episode: 223   steps: -1152.9439804\n",
      "223           -1282.88052461\n",
      "episode: 224   steps: -1169.67649817\n",
      "224           -1279.78527938\n",
      "episode: 225   steps: -1187.30324432\n",
      "225           -1276.57433911\n",
      "episode: 226   steps: -1173.00464785\n",
      "226           -1273.13015555\n",
      "episode: 227   steps: -1210.60720109\n",
      "227           -1270.09071233\n",
      "episode: 228   steps: -1304.0741911\n",
      "228           -1267.05241154\n",
      "episode: 229   steps: -1333.29038467\n",
      "229           -1265.17368539\n",
      "episode: 230   steps: -1352.65178831\n",
      "230           -1265.19984721\n",
      "episode: 231   steps: -1355.89245052\n",
      "231           -1265.59226374\n",
      "episode: 232   steps: -1406.71352915\n",
      "232           -1270.46108855\n",
      "episode: 233   steps: -1365.01291309\n",
      "233           -1270.23701436\n",
      "episode: 234   steps: -1218.74534069\n",
      "234           -1268.94061231\n",
      "episode: 235   steps: -1326.02976052\n",
      "235           -1269.54487811\n",
      "episode: 236   steps: -1171.1832124\n",
      "236           -1266.97664543\n",
      "episode: 237   steps: -1206.52163002\n",
      "237           -1264.58043566\n",
      "episode: 238   steps: -1009.43145128\n",
      "238           -1261.07227206\n",
      "episode: 239   steps: -1314.63342333\n",
      "239           -1260.02354512\n",
      "episode: 240   steps: -977.266204092\n",
      "240           -1256.4183945\n",
      "episode: 241   steps: -844.174140111\n",
      "241           -1251.97627927\n",
      "episode: 242   steps: -742.515667886\n",
      "242           -1246.29148798\n",
      "episode: 243   steps: -868.374901148\n",
      "243           -1243.27940531\n",
      "episode: 244   steps: -956.224178842\n",
      "244           -1239.62660364\n",
      "episode: 245   steps: -773.326071732\n",
      "245           -1233.4715364\n",
      "episode: 246   steps: -1005.90988536\n",
      "246           -1234.51865998\n",
      "episode: 247   steps: -1059.15099346\n",
      "247           -1231.10642102\n",
      "episode: 248   steps: -1011.39022539\n",
      "248           -1226.77501888\n",
      "episode: 249   steps: -1005.77120591\n",
      "249           -1223.20011795\n",
      "episode: 250   steps: -731.908343359\n",
      "250           -1215.7898606\n",
      "episode: 251   steps: -630.21324925\n",
      "251           -1210.86530675\n",
      "episode: 252   steps: -1142.29818355\n",
      "252           -1211.27211069\n",
      "episode: 253   steps: -973.519556249\n",
      "253           -1209.78384755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 254   steps: -1174.33441569\n",
      "254           -1210.20981643\n",
      "episode: 255   steps: -1135.22789187\n",
      "255           -1207.86582286\n",
      "episode: 256   steps: -1135.85162148\n",
      "256           -1206.38451708\n",
      "episode: 257   steps: -1188.06607091\n",
      "257           -1206.55166416\n",
      "episode: 258   steps: -1190.17190027\n",
      "258           -1206.23169255\n",
      "episode: 259   steps: -1144.28603429\n",
      "259           -1202.80677492\n",
      "episode: 260   steps: -1145.43830253\n",
      "260           -1202.7736265\n",
      "episode: 261   steps: -1208.60478896\n",
      "261           -1205.48443231\n",
      "episode: 262   steps: -1343.45308309\n",
      "262           -1208.96386508\n",
      "episode: 263   steps: -1209.97857303\n",
      "263           -1204.47050302\n",
      "episode: 264   steps: -1344.1260714\n",
      "264           -1202.41864186\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-b7a691fda37a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m                     \u001b[0mminibatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTD_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreplay_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmainDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargetDQN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mminibatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;31m#print (\"Loss :  \", loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-fbb23a1b1831>\u001b[0m in \u001b[0;36mreplay_train\u001b[0;34m(mainDQN, targetDQN, train_batch, w_batch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_batch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmainDQN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-e8ee6ff44527>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Qpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_X\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_stack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw_stack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    887\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 889\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    890\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1120\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1121\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1317\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1318\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1302\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
